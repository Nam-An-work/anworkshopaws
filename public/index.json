[
{
	"uri": "http://Nam-An-work.github.io/2-ingestandstore/2.1-s3/",
	"title": "2.1. Create S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Navigate to S3 Console \u0026amp; Create a new bucket in us-east-1 region :\nGo to S3 Console.\nClick - Create Bucket. Bucket Name - yourname-analytics-workshop-bucket Region - US EAST (N. Virginia). Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click - Create Bucket. Adding reference data\nOpen yourname-analytics-workshop-bucket Click - Create folder New folder called - data Click - Create folder Open - data Click - Create folder (From inside the data folder) New folder : reference_data Click - Create folder Open - reference_data Download this file locally : tracks_list.json In the S3 Console, Click - Upload Click Add files \u0026amp; upload the tracks_list.json file here Click Upload (bottom left) "
},
{
	"uri": "http://Nam-An-work.github.io/2-ingestandstore/2.2-kn/",
	"title": "2.2. Create Data Firehose",
	"tags": [],
	"description": "",
	"content": "In this step we will navigate to Data Console \u0026amp; create a Data Firehose delivery stream to ingest data \u0026amp; store in S3:\nGo to Kinesis Firehose Console. Click - Create delivery stream. Step 1: Choose source and destination Source: Direct PUT Destination: Amazon S3 Step 2: Delivery stream name Delivery stream name: analytics-workshop-stream Step 3: Transform and convert records Transform source records with AWS Lambda: Disabled (Leave \u0026lsquo;Turn on data transformation\u0026rsquo; as unchecked) RecConvert record format: Disabled (Leave \u0026lsquo;Enable record format conversion\u0026rsquo; as unchecked) Step 4: Destination settings S3 bucket: naman-analytics-workshop-bucket New line delimiter: Not Enabled Dynamic partitioning: Not Enabled S3 bucket prefix: data/raw/ S3 bucket error output prefix: Leave Blank Expand Buffer hints, compression and encryption Buffer size: 1 MiB Buffer interval: 60 seconds Compression for data records: Not Enabled Encryption for data records: Use the encryption setting of the S3 bucket Step 5: Advanced settings - Server-side encryption: unchecked - Amazon Cloudwatch error logging: Enabled - Permissions: Create or update IAM role KinesisFirehoseServiceRole-xxxx - Optionally add Tags, e.g.: workshop - AnalyticsOnAWS Step 6: Review Note: the slash / after raw is important. If you miss it Firehose will copy the data into an undesired location\n"
},
{
	"uri": "http://Nam-An-work.github.io/2-ingestandstore/2.3-dummy/",
	"title": "2.3. Generate Dummy Data",
	"tags": [],
	"description": "",
	"content": "In this step we will configure Kinesis Data Generator to produce fake data and ingest it into Kinesis Firehose.\nConfigure Amazon Cognito for Kinesis Data Generator - In this step we will launch a cloud formation stack that will configure Cognito. This cloudformation scripts launches in N.Virginia region (No need to change this region)\nGo to AWS Cloudformation Click - Next Specify Details: Username: admin Password: choose an alphanumeric password Click Next Options: Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click - Next Scroll down I acknowledge that AWS CloudFormation might create IAM resources: Check Click Next and Submit Refresh your AWS Cloudformation Console Wait till the stack status changes to Create_Complete Select the Kinesis-Data-Generator-Cognito-User stack Go to outputs tab: click on the link that says: KinesisDataGeneratorUrl - This will open your Kinesis Data Generator tool On Amazon Kinesis Data Generator homepage\nLogin with your username \u0026amp; password from previous step Region: us-east-1 Stream/delivery stream: analytics-workshop-stream Records per second: 2000 Record template (Template 1): In the big text area, insert the following json template: { \u0026quot;uuid\u0026quot;: \u0026quot;{{random.uuid}}\u0026quot;, \u0026quot;device_ts\u0026quot;: \u0026quot;{{date.utc(\u0026quot;YYYY-MM-DD HH:mm:ss.SSS\u0026quot;)}}\u0026quot;, \u0026quot;device_id\u0026quot;: {{random.number(50)}}, \u0026quot;device_temp\u0026quot;: {{random.weightedArrayElement( {\u0026quot;weights\u0026quot;:[0.30, 0.30, 0.20, 0.20],\u0026quot;data\u0026quot;:[32, 34, 28, 40]} )}}, \u0026quot;track_id\u0026quot;: {{random.number(30)}}, \u0026quot;activity_type\u0026quot;: {{random.weightedArrayElement( { \u0026quot;weights\u0026quot;: [0.1, 0.2, 0.2, 0.3, 0.2], \u0026quot;data\u0026quot;: [\u0026quot;\\\u0026quot;Running\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Working\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Walking\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Traveling\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Sitting\\\u0026quot;\u0026quot;] } )}} } Click - Send Data Once the tool sends ~10,000 messages, you can click on - Stop sending data to Kinesis Validate that data has arrived in S3\nAfter few moments go to the S3 console Navigate to: naman-analytics-workshop-bucket \u0026gt; data There should be a folder called raw \u0026gt; Open it and keep navigating, you will notice that firehose has dumped the data in S3 using yyyy/mm/dd/hh partitioning If you have received the dummy data in your s3 buckets, we are good to proceed to next step!\n"
},
{
	"uri": "http://Nam-An-work.github.io/3-catalogdata/3.1-iam/",
	"title": "3.1. Create Iam role",
	"tags": [],
	"description": "",
	"content": "In this step, we will access the IAM Console to create a new AWS Glue service role. This will grant AWS Glue the permissions to access data stored in S3 and create the required entities in the Glue Data Catalog.\nGo to Iam Click Create role Choose the service that will use this role: Glue Click Next Search for AmazonS3FullAccess Select the entry\u0026rsquo;s checkbox Search for AWSGlueServiceRole Select the entry\u0026rsquo;s checkbox Click Next Role name: AnalyticsworkshopGlueRole Make sure that only two policies attached to this role (AmazonS3FullAccess, AWSGlueServiceRole) Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click Create role "
},
{
	"uri": "http://Nam-An-work.github.io/3-catalogdata/3.2-glue/",
	"title": "3.2. Create AWS Glue Crawlers",
	"tags": [],
	"description": "",
	"content": "In this step, we will go to the AWS Glue Console and create Glue crawlers to discover the schema of the newly ingested data in S3.\nGo to AWS Glue On the left panel at Data Catalog, click on Crawlers Click on Create crawler, Crawler info: Crawler name: AnalyticsworkshopCrawler Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click Next Click Add a data source Choose a Data source: Data source: S3 Leave Network connection - optional Select In this account under Location of S3 data Include S3 path: s3://naman-analytics-workshop-bucket/data/ Leave Subsequent crawler runs to default selection of Crawl all sub-folders Click Add an S3 data source Select recently added S3 data source under Data Sources Click Next IAM Role Under Existing IAM role, select AnalyticsworkshopGlueRole Leave everything else as-is. Click Next Output configuration Click Add database to bring up a new window for creating a database Database details: Name: analyticsworkshopdb Click Create database Closes the current window and returns to the previous window. Refresh by clicking the refresh icon to the right of the Target database Choose analyticsworkshopdb under Target database Under Crawler schedule Frequency: On demand Click Next Review all settings under Review and create Click Create crawler You should see this message: The following crawler is now created: \u0026ldquo;AnalyticsworkshopCrawler\u0026rdquo; Click Run crawler to run the crawler for the first time Wait for few minutes Verify newly created tables in catalog Navigate to Glue Catalog and explore the crawled data: Go to AWS Glue Click analyticsworkshopdb Click Tables in analyticsworkshopdb Click raw Look around and explore the schema for your dataset Look for the averageRecordSize, recordCount, compressionType "
},
{
	"uri": "http://Nam-An-work.github.io/3-catalogdata/3.3-athena/",
	"title": "3.3. Query ingested data using Amazon Athena",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s query the newly ingested data using Amazon Athena\nGo to AWS Athena If necessary, click Edit seetings in the blue alert near the top of the Athena console Click Editor tab On the left panel (Database) drop down , select analyticsworkshopdb \u0026gt; select table raw Click on 3 dots (3 vertical dots) \u0026gt; Select Preview Table Review the output In query editor, paste the following query: SELECT activity_type, count(activity_type) FROM raw GROUP BY activity_type ORDER BY activity_type Click on Run "
},
{
	"uri": "http://Nam-An-work.github.io/4-datatransformation/4.1-glue-inter/",
	"title": "4.1. Transform Data with AWS Glue (interactive sessions)",
	"tags": [],
	"description": "",
	"content": "\nPrepare IAM policies and role In this step you will navigate to IAM console and create the necessary IAM policies and role to work with AWS Glue Studio Jupyter notebooks and interactive sessions. Let\u0026rsquo;s start by creating an IAM policy for the AWS Glue notebook role Go to Iam Click Policies from menu panel on the left Click Create policy Click on JSON tab Replace default text in policy editor window with the following policy statemenent. { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:PassRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::180294200626:role/Analyticsworkshop-GlueISRole\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;glue:*\u0026quot;, \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot;, \u0026quot;s3:GetBucketLocation\u0026quot;, \u0026quot;s3:ListAllMyBuckets\u0026quot;, \u0026quot;s3:GetLifecycleConfiguration\u0026quot;, \u0026quot;s3:GetBucketPolicy\u0026quot;, \u0026quot;s3:GetBucketCORS\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Note that Analyticsworkshop-GlueISRole is the role that we create for the AWS Glue Studio Jupyter notebook in next step.\nAlert: Replace with your AWS account ID in the copied policy statement.\nClick Next Policy Name: AWSGlueInteractiveSessionPassRolePolicy Optionally write description for the policy: Description: The policy allows AWS Glue notebook role to pass to interactive sessions so that the same role can be used in both places Tags Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Next, create an IAM role for AWS Glue notebook Go to Iam Click Roles from menu panel on the left Click Create Role Choose the service that will use this role: Glue under Use Case and Use cases for other AWS services Click Next Search for following policies and select the checkbox against them: AWSGlueServiceRole AwsGlueSessionUserRestrictedNotebookPolicy AWSGlueInteractiveSessionPassRolePolicy AmazonS3FullAccess Click Next Role name: Analyticsworkshop-GlueISRole Make sure only four policies are attached to this role (AWSGlueServiceRole, AwsGlueSessionUserRestrictedNotebookPolicy, AWSGlueInteractiveSessionPassRolePolicy, AmazonS3FullAccess) Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click Create role Note: We have granted full S3 access to the Glue role for the purpose of this workshop. It is recommended to grant only the permissions required to perform a task i.e. follow least-privilege permissions model in real/actual deployments.\nUse Jupyter Notebook in AWS Glue for interactive ETL development In this step you will be creating an AWS Glue job with Jupyter Notebook to interactively develop Glue ETL scripts using PySpark.\nDownload and save this file locally on your laptop: analytics-workshop-glueis-notebook.ipynb Go to: Glue Studio jobs Select Jupyter Notebook option Select Upload and edit an existing notebook Click Choose file Browse and upload analytics-workshop-glueis-notebook.ipynb which you downloaded earlier Click Create Under Notebook setup and Initial configuration Job name: AnalyticsOnAWS-GlueIS IAM role Analyticsworkshop-GlueISRole Leave Kernel to default as Spark Click Start notebook Once the notebook is initialized, follow the instructions in the notebook Validate - Transformed / Processed data has arrived in S3 Once the ETL script has ran successfully, return to the console: Click me\nClick - naman-analytics-workshop-bucket \u0026gt; data Open the processed-data folder: Ensure that .parquet files have been created in this folder. Now that we have transformed the data, we can query the data using Amazon Athena. We could also further transform/aggregate the data with AWS Glue or Amazon EMR. The next module on EMR is optional. You could skip it if you want to and proceed to Analyze with Athena. "
},
{
	"uri": "http://Nam-An-work.github.io/4-datatransformation/4.2-glue-grap/",
	"title": "4.2. Transform Data with AWS Glue Studio (graphical interface)",
	"tags": [],
	"description": "",
	"content": "\nWhat is AWS Glue Studio? AWS Glue Studio is a new graphical interface that makes it easy to create, run, and monitor extract, transform, and load (ETL) jobs in AWS Glue. You can visually compose data transformation workflows and seamlessly run them on AWS Glue’s Apache Spark-based serverless ETL engine.\nIn this lab, We will do the same ETL process like Transform Data with AWS Glue (interactive sessions)\nBut This time We will leverage visual graphical interface in AWS Glue Studio!\nPractice Go to Glue Studio Console Click - Jobs and choose Visual with a blank canvas Click Create Click - Source and choose - S3 Click tab Data source properties - S3 in configuration window on right side of the screen. Under S3 source type choose Data Catalog table Database - analyticsworkshopdb Table - raw Now lets repeat same steps to add reference_data from S3. Click - Source and choose - S3 Click tab Data source properties - S3 in configuration window on right side of the screen. Under S3 source type choose Data Catalog table Database - analyticsworkshopdb Table - reference_data Click the S3 node added earlier in the canvas, then click add node and choose Change Schema. Select the Transform tab on the right and change the Data type of track_id to int. Click the left S3 node in the canvas, then click add node and choose Join You should get the visual diagram like in the screenshot below, and message on the right \u0026ldquo;Insufficient source nodes\u0026rdquo; because you need another node (Data source) to join Next, click on Transform - Join node and on Node properties on the right configuration window, select dropdown list and and select Change Schema under Tramsforms as depicted in the screenshot below: Click Add condition. Choose track_id column as join column as shown in the screenshot below. With Join node selected on canvas, click add node and choose Change Schema You should get visual diagram like in the screenshot below We will drop unused columns and map new data type for following columns: drop Columns: parition_0 parition_1 parition_2 parition_3 Map New Data Type: track_id string Click Transform - ApplyMapping node on the canvas Click Target and choose S3 as shown in the screenshot below In Data target properties - S3, provide inputs as following: Format: Parquet Compression Type: Snappy S3 Target Location: s3://naman-analytics-workshop-bucket/data/processed-data2/ Data Catalog update options Choose Create a table in the Data Catalog and on subsequent runs, update the schema and add new partitions Database: analyticsworkshopdb Table name: processed-data2 Click Job details and configure with following options Name: AnalyticsOnAWS-GlueStudio IAM Role: AnalyticsWorkshopGlueRole Requested number of workers: 2 Job bookmark: Disable Number of retries: 1 Job timeout (minutes): 10 Leave the rest as default value Click Save Click Save and you should see \u0026ldquo;Successfully created job\u0026rdquo;. Start this ETL job by clicking Run on upper right hand corner of the screen. Wait for a few seconds and you should see your ETL job Run Status \u0026ldquo;Succeeded\u0026rdquo; as shown in the screenshot below. You can see the Pyspark Code that Glue Studio has generated and reuse this code for other purposes, if needed. Go to Glue DataCatalog: Click me and you should see processed-data2 table created under analyticsworkshopdb database Well Done!! You have finished an extra ETL lab with AWS Glue Studio. With AWS Glue Studio, you can visually compose data transformation workflows and seamlessly run them on AWS Glue’s Apache Spark-based serverless ETL engine. "
},
{
	"uri": "http://Nam-An-work.github.io/4-datatransformation/4.3-glue-data/",
	"title": "4.3. Transform Data with AWS Glue DataBrew",
	"tags": [],
	"description": "",
	"content": "\nWhat is AWS Glue DataBrew AWS Glue DataBrew is a new visual data preparation tool that makes it easy for data analysts and data scientists to clean and normalize data to prepare it for analytics and machine learning. You can choose from over 250 pre-built transformations to automate data preparation tasks, all without the need to write any code. You can automate filtering anomalies, converting data to standard formats, and correcting invalid values, and other tasks. After your data is ready, you can immediately use it for analytics and machine learning projects. You only pay for what you use - no upfront commitment.\nPractice Go To Glue Databrew Console Click - Create project Enter Project name: AnalyticsOnAWS-GlueDataBrew as shown in the screenshot below Under Select a dataset choose New dataset In New dataset details, enter raw-dataset as Dataset name as shown in the screenshot below. Under Connect to new dataset, select All AWS Glue tables , you should see all databases in AWS Glue Catalog as shown in the screenshot below Click analyticsworkshopdb, choose raw table Under Permissions Choose Role name as Create new IAM role Enter AnalyticsOnAWS-GlueDataBrew in New IAM role suffix Click Create project Once Glue DataBrew session is created, you should see like in the screenshot below: Click SCHEMA tab on the right hand top corner of the screen to explore table schema and its properties such as column name, data type, data quality, value distribution, and box plot distribution for numeric values. Click GRID tab to return to grid view, We will change track_id data type, by click # in track_id column, and choose string type as shown in the following screenshot Now, lets profile the data to unearth informative statistics like correlation between attributes. Click PROFILE tab on the right hand top corner of the screen, and click Run data profile Leave Job name, and Job run sample as default option Specify S3 location as your bucket name s3://naman-analytics-workshop-bucket/ for job output. Do not forget to replace portion in the s3 path. Leave Enable encryption for job output file option unchecked. Under Permissions, select the role as Role name that was created previously in this module. Click Create and run job You should get similar output as shown in the screenshot below which means that Glue Databrew has already started profiling your data Click GRID tab to return to grid view Click Join Click Connect new dataset Click All AWS Glue tables, and click analyticsworkshopdb Click reference_data Dataset name - reference-data-dataset You should see similar screen as shown in the screenshot below, click Next Select track_id from raw-dataset Select track_id from reference-data-set Unselect track_id from Table B Click Finish You should see result as showin in the screenshot below: Click PROFILE to review your raw dataset profiling result such as summary, missing cells, duplicate rows, correlations, value distribution, and columns statistics, it will give you deeper level of insights about your data Click LINEAGE on the top right corner You should be able to see the data lineage, that is visually represented to provide understanding of data flow with involved transformation at all steps from source to sink. Go back to the GRID view, and click Create job Fill in the following values: Under Job details Job name: **AnalyticsOnAWS-GlueDataBrew-Job Under Job output settings File type: GlueParquet S3 location: s3://naman-analytics-workshop-bucket/data/processed-data/ Scroll down to Permission, and choose role name that you have created in first step, and click Create and run job You should see 1 job in progress Click Jobs on the left menu, you should see following screenshot, then click Job name (Hyperlink) Here you can explore job run history, job detail, and data lineage like in the screeshot below: This job should take around 4-5 minutes to be completed, you should see Succeeded status, and click 1 output under Output columnn. You should be able to see output destination under Destination column, click on S3 location (hyperlink) provided in this column You should see output file from Glue Databrew job! "
},
{
	"uri": "http://Nam-An-work.github.io/4-datatransformation/4.4-emr/",
	"title": "4.4. Transform Data with EMR",
	"tags": [],
	"description": "",
	"content": "\n1. Copy the script to S3 In this step, we will navigate to S3 Console and create couple of folders to be used for the EMR step.\nGo to: S3 Console Add the PySpark script: Open naman-analytics-workshop-bucket Click Create folder Create a new folder called scripts Click Create folder Open scripts Download this file locally In the S3 console, click Upload Create a folder for EMR logs: Open naman-analytics-workshop-bucket Click Create folder Create a new folder called logs Click Create folder 2. Create EMR cluster and add step In this step, we will create a EMR cluster and submit a Spark step.\nGo to to the EMR console Click on Create cluster Name and applications: Name: analytics-workshop-transformer Amazon EMR release: default (e.g.: emr-6.10.0) Application bundle: Spark AWS Glue Data Catalog settings: Use for Spark table metadata unchecked Leave the other settings to default Cluster configuration: Choose Instance groups Leave Primary, Core and Task to default value (m5.xlarge) Leave Cluster scaling and provisioning option to default (Core: size 1, Task -1: size 1) Networking leave to default Step: -Add Step Type: Spark application Name: Spark job Deploy mode: Cluster mode Application location: s3://naman-analytics-workshop-bucket/scripts/emr_pyspark.py Arguments: enter the name of your s3 bucket naman-analytics-workshop-bucket Action if step fails: Terminate cluster Click Save step Cluster termination: Automatically terminate cluster after idle time (Recommended) Idle time: 0 days 01:00:00 Check Terminate cluster after last step completes Uncheck Use termination protection Cluster logs: Check Publish cluster-specific logs to Amazon S3 Amazon S3 location: s3://naman-analytics-workshop-bucket/logs/ Tags: Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Identity and Access Management (IAM) roles Amazon EMR service role: Create a service role EC2 instance profile for Amazon EMR: Create an instance profile S3 bucket access: All S3 buckets in this account with read and write access Click Create cluster 3. Check the status of the Transform Job run on EMR The EMR cluster will take 6-8 minute to get provisioned, and another minute or so to complete the Spark step execution. The Cluster will be terminated after the Spark job has been executed. To check the status of the job, click on the Cluster name: analytics-workshop-transformer Go to the Steps tab Here you should see two items: Spark application and Setup hadoop debugging The status of the Spark application should change from Pending to Running to Completed Once the Spark job run is complete the EMR cluster will be terminated Under EMR \u0026gt; Cluster, you will see the Status of the cluster as Terminated with All steps completed message. 4. Validate - Transformed / Processed data has arrived in S3 Let\u0026rsquo;s go ahead and confirm that the EMR transform job has created the data set in the S3 console - Click - naman-analytics-workshop-bucket \u0026gt; data - Open the new folder emr-processed-data - Ensure that .parquet files are created in this folder. 5. Rerun the Glue Crawler Go to: Glue console\nOn the left panel, click on Crawlers Select the crawler created in the previous module: AnalyticsworkshopCrawler Click on Run You should see the Status change to Starting Wait for few minutes for the crawler run to complete The crawler to display the Tables added as 1 You can go to the databases section on the left and confirm that emr_processed_data table has been added. You will now be able to query the results of the EMR job using Amazon Athena in the next module.\n"
},
{
	"uri": "http://Nam-An-work.github.io/6-analyzewithkinesis/6.1-iam/",
	"title": "6.1 Create Iam Role",
	"tags": [],
	"description": "",
	"content": " Go to AWS IAM Role Click Create role Choose Kinesis service under Use case from drop down menu that says Use cases for other AWS services: Chose Kinesis Analytics Click Next for Permissions Search for AWSGlueServiceRole and select the entry\u0026rsquo;s checkbox Search for AmazonKinesisFullAccess and select the entry\u0026rsquo;s checkbox Click Next Enter Role name: AnalyticsworkshopKinesisAnalyticsRole Make sure that only two policies are attached to this role (AWSGlueServiceRole, AmazonKinesisFullAccess) Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click Create Role "
},
{
	"uri": "http://Nam-An-work.github.io/6-analyzewithkinesis/6.2-kin/",
	"title": "6.2. Create Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Kinesis Data Generator is an application that makes it simple to send test data to your Amazon Kinesis stream or Amazon Kinesis Firehose delivery stream. We will create Kinesis Data Stream to ingest the data from Kinesis Data Generator. Our Kinesis Application notebook will read streaming data from this Kinesis Data Stream.\nGo to AWS Kinesis service Click Create Data Stream Enter a data stream name: analytics-workshop-data-stream In Data Stream capacity: Choose Capacity Mode: Provisioned Set Provisioned Shards: 2 Go down to bottom and click Create Data Stream "
},
{
	"uri": "http://Nam-An-work.github.io/6-analyzewithkinesis/6.3-glue/",
	"title": "6.3. Create Glue Catalog Table",
	"tags": [],
	"description": "",
	"content": "Our Kinesis Application Notebook will get information about the data source from AWS Glue. When you create your Studio notebook, you specify the AWS Glue database that contains your connection information. When you access your data sources, you specify AWS Glue tables contained in the database.\nGo to AWS glue From the left sidebar, go to Databases and click our previously created database analyticsworkshopdb Click Tables in analyticsworkshopdb Click Add tables drop down menu and then select Add table manually \\ In Table Properties\nEnter Table Name: raw_stream In Data store\nSelect the type of source: Kinesis Skip Select a kinesis data stream. (Stream in my account should be selected by default) Region: US East (N. Virginia) us-east-1 Kinesis stream name: analytics-workshop-data-stream Skip sample size In Data format\nClassification: JSON Then Click Next In Schema\nClick Edit Schema as JSON and insert the following json text: [{ \u0026quot;Name\u0026quot;: \u0026quot;uuid\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_ts\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_id\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_temp\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;track_id\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;activity_type\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }] Then Click Next Skip Partition Indices Review and create\nVerify that the new Glue table raw_stream is properly created. Refresh the table list if it has not shown up yet. Click the newly created table raw_stream Click Actions and click Edit table Under Table properties, add new property: Key: kinesisanalytics.proctime Value: proctime "
},
{
	"uri": "http://Nam-An-work.github.io/6-analyzewithkinesis/6.4-stu/",
	"title": "6.4. Create Analytics Streaming Application Studio Notebook",
	"tags": [],
	"description": "",
	"content": "Create Analytics Streaming Application Studio Notebook Now let\u0026rsquo;s create our Kinesis Analytics Streaming Application Studio Notebook. This Analytics application notebook can process streaming data from Kinesis Data Stream and we can write SQL analytical queries to get real-time insights such as current activity count or device temperature.\nGo to Managed Apache Flink Navigate to Studio Click Create Studio notebook Choose Create with custom settings General Enter Studio notebook name: AnalyticsWorkshop-KDANotebook Enter Runtime: Apache Flink 1.15, Apache Zeppelin 0.10 Click Next IAM role Select Choose from IAM roles that Kinesis Data Analytics can assume Select our previously created service role: AnalyticsworkshopKinesisAnalyticsRole In AWS Glue database, select: analyticsworkshopdb Click Next In Configurations Parallelism: 4 Parallelism per KPU: 1 Uncheck the Turn on logging checkbox Skip everything else and go to the bottom, in tag: workshop: AnalyticsOnAWS Running the Kinesis Analytics Studio Notebook Now that we have created our notebook, we can run it and try to execute some SQL queries.\nSee our list of notebooks in Studio tab and click our newly created notebook AnalyticsWorkshop-KDANotebook Click Run Wait until the Status goes into Running mode. (It will take about 5-7 minutes) Click Open in Apache Zeppelin at the upper right hand side to open Zeppelin Notebook Click Create new note and give the note a name AnalyticsWorkshop-ZeppelinNote Paste this SQL query %flink.ssql(type=update) SELECT * FROM raw_stream; Click Add Paragraph Paste this SQL query %flink.ssql(type=update) SELECT activity_type, count(*) as activity_cnt FROM raw_stream group by activity_type; When all queries are pasted, Click the Play button at the top right of the paragraph NOTE: There will be no data shown yet because this Streaming Analytics notebook processes streaming data. We have to send streaming data from our Kinesis Data Generator (if not already), so the notebook can display the result of the queries Generate Dummy Data to Kinesis Data Stream To display data from queries run in Analytics Streaming notebook, we have to send the raw data from our Kinesis Data Generator.\nGo to the KinesisDataGeneratorURL. You can find this in the Cloudformation stack\u0026rsquo;s Output tab. Login with your username \u0026amp; password Fill this out: Region: us-east-1 Stream/delivery stream: analytics-workshop-data-stream (DO NOT choose analytics-workshop-stream that you might have created in \u0026ldquo;Ingest and Store\u0026rdquo; module of this workshop) Ensure Records per second is Constant. Value for Records per second: 100 (DO NOT change this number for the workshop.) Ensure that Compress Records is unchecked. Record template (Template 1): In the big text area, insert the following json template: { \u0026quot;uuid\u0026quot;: \u0026quot;{{random.uuid}}\u0026quot;, \u0026quot;device_ts\u0026quot;: \u0026quot;{{date.utc(\u0026quot;YYYY-MM-DD HH:mm:ss.SSS\u0026quot;)}}\u0026quot;, \u0026quot;device_id\u0026quot;: {{random.number(50)}}, \u0026quot;device_temp\u0026quot;: {{random.weightedArrayElement( {\u0026quot;weights\u0026quot;:[0.30, 0.30, 0.20, 0.20],\u0026quot;data\u0026quot;:[32, 34, 28, 40]} )}}, \u0026quot;track_id\u0026quot;: {{random.number(30)}}, \u0026quot;activity_type\u0026quot;: {{random.weightedArrayElement( { \u0026quot;weights\u0026quot;: [0.1, 0.2, 0.2, 0.3, 0.2], \u0026quot;data\u0026quot;: [\u0026quot;\\\u0026quot;Running\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Working\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Walking\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Traveling\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Sitting\\\u0026quot;\u0026quot;] } )}} } Then Click Send Data Go back to our Zeppelin Notebook. Wait for a few minutes, the result should be displayed "
},
{
	"uri": "http://Nam-An-work.github.io/1-introduce/",
	"title": "1. Introduction",
	"tags": [],
	"description": "",
	"content": "Achievements from this Architecture Design and Build Data Lake Architecture Design a serverless data lake architecture Build a data processing pipeline and Data Lake using Amazon S3 for data storage Real-time Data Streaming and Analysis Stream real-time data using Amazon Kinesis Analyze real-time data with Amazon Kinesis Data Analytics Data Management and Processing Data Management and Processing Efficiently transform and process data Run interactive ETL scripts in Jupyter notebook on AWS Glue Studio using AWS Glue interactive sessions ETL Job Management and Monitoring Monitor and manage ETL jobs through Glue Studio Data Preparation and Transformation Easily prepare data with Glue DataBrew Run Spark transformation jobs on EMR Data Loading and Optimization in Data Warehouse Load data from Glue into Amazon Redshift Apply best design practices for Amazon Redshift Data Querying and Visualization Query and analyze data with Amazon Athena Visualize results using Amazon QuickSight Pre-requisites You need to have access to an AWS account with AdminstratorAccess This lab should be executed in us-east-1 region "
},
{
	"uri": "http://Nam-An-work.github.io/2-ingestandstore/",
	"title": "2. Ingest and Store",
	"tags": [],
	"description": "",
	"content": " To begin, we’ll generate some sample data in near real-time using the Kinesis Data Generator tool and stream it to Amazon S3 via Kinesis Firehose delivery stream.\nAdditionally, we will directly upload some reference data into an Amazon S3 bucket.\nTo begin, login to AWS Console in us-east-1 region: Click me\nContent 2.1. Create S3 Bucket 2.2. Create Kinesis Firehose 2.3. Generate Dummy Data\n"
},
{
	"uri": "http://Nam-An-work.github.io/3-catalogdata/",
	"title": "3. Catalog Data",
	"tags": [],
	"description": "",
	"content": " Next, we are going to register the datasets in the AWS Glue Data Catalog. We will automate the metadata capture with the help of Glue Crawlers.\nOnce the catalog entities are created, we will able to start querying the raw format of the data from Amazon Athena.\nContent 3.1. Create Iam role 3.2. Create AWS Glue Crawlers 3.3. Query ingested data using Amazon Athena\n"
},
{
	"uri": "http://Nam-An-work.github.io/4-datatransformation/",
	"title": "4. Data Transformation",
	"tags": [],
	"description": "",
	"content": "Now that we have cataloged the data, lets proceed to the next step of transforming the data using AWS Glue ETL!\nContent 4.1. Transform Data with AWS Glue (interactive sessions) 4.2. Transform Data with AWS Glue Studio (graphical interface) 4.3. Transform Data with AWS Glue DataBrew 4.4. Transform Data with EMR\n"
},
{
	"uri": "http://Nam-An-work.github.io/5-analyzewithathena/",
	"title": "5. Analyze with Athena",
	"tags": [],
	"description": "",
	"content": " In this step we will analyze the transformed data using Amazon Athena. Login to the Amazon Athena Console.\nGo to: Athena Console As Athena uses the AWS Glue catalog for keeping track of data source, any S3 backed table in Glue will be visible to Athena. On the left panel, select \u0026lsquo;analyticsworkshopdb\u0026rsquo; from the drop down Run the following query: SELECT artist_name, count(artist_name) AS count FROM processed_data GROUP BY artist_name ORDER BY count desc Explore the Athena UI and try running some queries. Try querying the emr_processed_data table. This query returns the list of tracks repeatedly played by devices. SELECT device_id, track_name, count(track_name) AS count FROM processed_data GROUP BY device_id, track_name ORDER BY count desc Preview table "
},
{
	"uri": "http://Nam-An-work.github.io/6-analyzewithkinesis/",
	"title": "6. Analyze with Kinesis Data Analytics",
	"tags": [],
	"description": "",
	"content": " In previous section, you have explored how to analyze data using Amazon Athena. In this section, we will see how to perform real-time analysis of streaming data using Amazon Kinesis Data Analytics . This can be done in 2 ways, using legacy SQL Application or using the newer recommended Studio notebook. In this workshop we will use Studio notebook and create SQL-based Kinesis Analytics Application.\nContent 6.1. Create Iam Role 6.2. Create Kinesis Data Stream\n6.3. Create Glue Catalog Table\n6.4. Create Analytics Streaming Application Studio Notebook\n"
},
{
	"uri": "http://Nam-An-work.github.io/7-quicksight/",
	"title": "7. Visualize in Quicksight",
	"tags": [],
	"description": "",
	"content": " In this module, we are going use Amazon Quicksight to build a few visualizations over the data collected and stored in S3.\nSetting Up QuickSight In this step we will visualize our processed data using QuickSight.\nGo to: Quicksight Console Click Sign up for QuickSight Ensure Enterprise is selected and click Continue QuickSight account name: yournameanalyticsworkshop Notification email address: you@youremail.com Select Amazon Athena - this enables QuickSight access to Amazon Athena databases Select Amazon S3 Select naman-analytics-workshop-bucket Click Finish Wait for your QuickSight account to be created Adding a New Dataset Go to quicksight On top right, click New Analyze On top left, click New Dataset Click Athena New Athena data source Data source name: analyticsworkshop Click Validate connection Click Create data source Choose your table Database: contain sets of tables - select analyticsworkshopdb Tables: contain the data you can visualize - select processed_data Click Select Finish data set creation Select Directly query your data Click Visualize Using Amazon Quick Sight to Visualize Our Processed Data On the bottom-left panel (Visual types):\nHover on icons there to see names of the available visualizations Click on Heat Map On top-left panel (Fields list) Click device_id Click track_name Just above the visualization you should see Field wells: Rows: device_id | Columns: track_name "
},
{
	"uri": "http://Nam-An-work.github.io/8-servewithlambda/",
	"title": "8. Serve with Lambda",
	"tags": [],
	"description": "",
	"content": "\nCreating a Lambda Function Go to: Lambda Console Click Create function Select Author from scratch Under Basic Information Give Function name as Analyticsworkshop_top5Songs Select Runtime as Python 3.9 Expand Choose or create an execution role under Permissions, make sure Create a new role with basic Lambda permissions is selected. Scroll down to Function Code section and replace existing code under in lambda_function.py with the python code below: import boto3 import time import os # Environment Variables DATABASE = os.environ['DATABASE'] TABLE = os.environ['TABLE'] # Top X Constant TOPX = 5 # S3 Constant S3_OUTPUT = f's3://{os.environ[\u0026quot;BUCKET_NAME\u0026quot;]}/query_results/' # Number of Retries RETRY_COUNT = 10 def lambda_handler(event, context): client = boto3.client('athena') # query variable with two environment variables and a constant query = f\u0026quot;\u0026quot;\u0026quot; SELECT track_name as \\\u0026quot;Track Name\\\u0026quot;, artist_name as \\\u0026quot;Artist Name\\\u0026quot;, count(1) as \\\u0026quot;Hits\\\u0026quot; FROM {DATABASE}.{TABLE} GROUP BY 1,2 ORDER BY 3 DESC LIMIT {TOPX}; \u0026quot;\u0026quot;\u0026quot; response = client.start_query_execution( QueryString=query, QueryExecutionContext={ 'Database': DATABASE }, ResultConfiguration={'OutputLocation': S3_OUTPUT} ) query_execution_id = response['QueryExecutionId'] # Get Execution Status for i in range(0, RETRY_COUNT): # Get Query Execution query_status = client.get_query_execution( QueryExecutionId=query_execution_id ) exec_status = query_status['QueryExecution']['Status']['State'] if exec_status == 'SUCCEEDED': print(f'Status: {exec_status}') break elif exec_status == 'FAILED': raise Exception(f'STATUS: {exec_status}') else: print(f'STATUS: {exec_status}') time.sleep(i) else: client.stop_query_execution(QueryExecutionId=query_execution_id) raise Exception('TIME OVER') # Get Query Results result = client.get_query_results(QueryExecutionId=query_execution_id) print(result['ResultSet']['Rows']) # Function can return results to your application or service # return result['ResultSet']['Rows'] Environment Variables Scroll down to Environment variables section and add below three Environment variables.\nKey: DATABASE, Value: analyticsworkshopdb Key: TABLE, Value: processed_data Key: BUCKET_NAME, Value: yourname-analytics-workshop-bucket Leave the Memory (MB) as default which is 128 MB Change Timeout to 10 seconds. Execution Role Select the Permissions Tab at the top: Click the Role Name link under Execution Role to open the IAM Console in a new tab. Click Add permissions and click Attach policies Add the following two policies (search in filter box, check and hit Attach policy): AmazonS3FullAccess AmazonAthenaFullAccess Once these policies are attached to the role, close this tab. Configuring The Test Event Our function is now ready to be tested. Deploy the function first by clicking on Deploy under the Function code section. Next, let\u0026rsquo;s configure a dummy test event to see execution results of our newly created lambda function. Click Test on right top hand corner of the lambda console.\nA new window will pop up for us to configure test event. Create new test event is selected by default. Event name: Test Template: Hello World Leave everything as is Click Save Click Test again You should be able to see the output in json format under Execution Result section. Verification through Athena Go to: Athena Console On the left panel, select analyticsworkshopdb from the dropdown Run the following query SELECT track_name as \u0026quot;Track Name\u0026quot;, artist_name as \u0026quot;Artist Name\u0026quot;, count(1) as \u0026quot;Hits\u0026quot; FROM analyticsworkshopdb.processed_data GROUP BY 1,2 ORDER BY 3 DESC LIMIT 5; "
},
{
	"uri": "http://Nam-An-work.github.io/9-warehouseonredshift/",
	"title": "9. Warehouse on Redshift",
	"tags": [],
	"description": "",
	"content": " In this module, we are going to setup an Amazon Redshift cluster, and use AWS Glue to load the data into Amazon Redshift. We will learn about several design considerations and best practices on creating and loading data into tables in Redshift, and running queries against it.\nCreate Redshift IAM Role Go to: Redshift Console\nClick on Create role Select Redshift under Use case and Use cases for other AWS services: Select Redshift - customizable Click Next In Search box, search and check following two policies AmazonS3FullAccess AWSGlueConsoleFullAccess Click Next Give Role Name as Analyticsworkshop_RedshiftRole Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Click Create role Create Redshift cluster Go to Redshift cluster\nClick Provision and manage clusters from menu panel on the left Click Create Cluster Leave Cluster identifier as redshift-cluster-1 Select dc2.large as Node Type Select Number of Nodes as 2. Verify Configuration summary. Change Master user name to admin Enter Master user password Under Cluster permissions Click Associate IAM role Select previously created Analyticsworkshop_RedshiftRole Click Associate IAM roles Analyticsworkshop_RedshiftRole should appear under Associated IAM roles Leave Additional configurations to default. It uses default VPC and default security group. Click Create cluster Create S3 Gateway Endpoint Go to: AWS VPC Console\nClick Create endpoint Name tag - optional: RedshiftS3EP Select AWS Services under Service category (which is the default selection) Under Service name search box, search for \u0026ldquo;s3\u0026rdquo; and hit enter/return. com.amazonaws.us-east-1.s3 should come up as search result. Select this option with type as Gateway Under VPC, chose default VPC. This is the same VPC which was used for configuring redshift cluster Once you have double checked VPC id, move to configuring Route tables section. Select the listed route table (this should be the main route table). You can verify this by checking Yes under Main column. Leave Policy as default. (Full access) Optionally add Tags, e.g.: * workshop: AnalyticsOnAWS Click Create endpoint. It should take a couple of seconds to provision this. Once this is ready, you should see Status as - Available against the newly created S3 endpoint. Verify and add rules to the default security group Go to: VPC Security Groups\nSelect the Redshift security group. It should be the default security if it was not changed during the Redshift cluster creation step. Click Edit inbound rules Click Edit outbound rules Create Redshift connection under Glue Connection. Go to: Glue Connections Console\nUnder Connections Click Create connection Give Connection name as analytics_workshop. Choose Connection type as Amazon Redshift. Setup Connections access Choose Database instances as redshift-cluster-1 Database name and Username should get populated automatically as dev and admin respectively. Password: Enter the password which you have used during Redshift cluster setup. Click Create connection Create schema and redshift tables. Go to: Redshift Query Editor\nClick Connect to database\nConnection: Create a new connection\nAuthentication: Temporary credentials\ncluster: redshift-cluster-1\nDatabase name: dev\nDatabase user: admin\nClick Connect Execute the queries below to create schema and tables for raw and reference data. CREATE schema redshift_lab; -- Create f_raw_1 table. CREATE TABLE IF not EXISTS redshift_lab.f_raw_1 ( uuid varchar(256), device_ts timestamp, device_id int, device_temp int, track_id int, activity_type varchar(128), load_time int ); -- Create d_ref_data_1 table. CREATE TABLE IF NOT EXISTS redshift_lab.d_ref_data_1 ( track_id int, track_name varchar(128), artist_name varchar(128) ); Use Jupyter Notebook in AWS Glue for interactive ETL development Download and save this file locally on your laptop file\nGo to: Glue Studio jobs Select Jupyter Notebook option Select Upload and edit an existing notebook Click Choose file Browse and upload analytics-workshop-redshift-glueis-notebook.ipynb which you downloaded earlier Click Create Under Notebook setup and Initial configuration Job name: AnalyticsOnAWS-Redshift IAM role Analyticsworkshop-GlueISRole Leave Kernel to default as Spark Click Start notebook Execute the following queries to check the number of records in raw and reference data table. select count(1) from redshift_lab.f_raw_1; select count(1) from redshift_lab.d_ref_data_1; select track_name, artist_name, count(1) frequency from redshift_lab.f_raw_1 fr inner join redshift_lab.d_ref_data_1 drf on fr.track_id = drf.track_id where activity_type = 'Running' group by track_name, artist_name order by frequency desc limit 10; "
},
{
	"uri": "http://Nam-An-work.github.io/",
	"title": "Optimizing Data Pipelines for Real-time Analysis and Transformation on AWS",
	"tags": [],
	"description": "",
	"content": "Optimizing Data Pipelines for Real-time Analysis and Transformation on AWS Overall The workshop will provide a comprehensive overview of building an analytics platform on AWS, covering each stage of the data processing pipeline. It will explore how to efficiently ingest, store, transform, and analyze data by utilizing a range of powerful AWS services. Key services such as AWS Glue for data preparation, Amazon Athena for querying data, Amazon Kinesis for real-time data streaming, Amazon EMR for big data processing, and Amazon QuickSight for data visualization will be discussed. Additionally, AWS Lambda and Amazon Redshift will be introduced for serverless computing and data warehousing, respectively, offering scalable solutions for building robust analytics systems.\nContent Introduction Ingest and store Catalog Data Data Transformation Analyze with Athena Analyze with Kinesis Data Analytics Quícksight Serve with Lambda Warehouse on Redshift Clean up resources "
},
{
	"uri": "http://Nam-An-work.github.io/10-cleanup/",
	"title": "10. Clean up resources  ",
	"tags": [],
	"description": "",
	"content": "We will take the following steps to delete the resources we created in this exercise.\nKinesis Firehose Delivery Stream Go to: Kinesis Console Click me Delete Firehose: analytics-workshop-stream Kinesis Data Stream Go to: Kinesis Console Click me\nDelete Data Stream: analytics-workshop-data-stream Kinesis Data Analytics Studio Notebook Go to: Kinesis Console Click me Delete Notebook: AnalyticsWorkshop-KDANotebook Lambda Go to: Lambda Console Click me Navigate to list of functions and select Analyticsworkshop_top5Songs. Under Actions drop down menu, select Delete. Glue Database Go to: Glue Console Click me Delete Database: analyticsworkshopdb Glue Crawler Go to: Glue Crawlers Click me Delete Crawler: AnalyticsworkshopCrawler Glue Studio Job GoTo: Glue Studio Job Click me\nCheck AnalyticsOnAWS-GlueStudio Check AnalyticsOnAWS-GlueIS Check AnalyticsOnAWS-Redshift Click Action and choose Delete job(s) Glue DataBrew projects GoTo: Glue DataBrew projects Click me Check AnalyticsOnAWS-GlueDataBrew Click Action and choose Delete Check Delete attached receipe and click Delete Glue DataBrew datasets GoTo: Glue DataBrew datasetsClick me Check dataset name: reference-data-dataset and raw-dataset Click Action and choose Delete Confirm deletion by clicking Delete Glue DataBrew Jobs GoTo: Glue DataBrew JobsClick me Check dataset name: raw-dataset profile job Click Action and choose Delete Confirm deletion by clicking Delete Delete Glue connection Go to: Glue Connections Click me Select analytics_workshop From Actions select Delete Connection Click Delete Delete IAM Role Go to: IAM Console Click me Search for following roles and delete: Analyticsworkshop-GlueISRole Analyticsworkshop_RedshiftRole AnalyticsworkshopKinesisAnalyticsRole Analyticsworkshop_top5Songs-role- Delete IAM Policy Go to: IAM Console Click me Search for following policies and delete: AWSGlueInteractiveSessionPassRolePolicy Delete Redshift cluster Go to: Redshift Console Click me Select redshift-cluster-1 From Actions menu, select Delete. Uncheck Create final snapshot Click Delete Delete S3 Gateway Endpoint Go to: S3 Gateway Endpoints Click me Select RedshiftS3EP From Actions select Delete Endpoint Revert Security Group rules Go to: EC2 Security Groups Click me For the default security group: Click on Inbound Rules Click Edit Rules Delete the row with S3 prefix list ID Click Save rules Click on Outbound rules Click Edit Rules Delete the self-referencing All TCP rule. Click Save rules Delete S3 bucket Go to: S3 Console Click me Delete Bucket: yourname-analytics-workshop-bucket You may need to first Empty the bucket as prompted Once emptied, proceed to Delete the bucket Delete the Cognito CloudFormation Stack Go to: CloudFormation Click me Click: Kinesis-Data-Generator-Cognito-User Click: Actions \u0026gt; DeleteStack On confirmation screen: Click Delete Close QuickSight account Go to: Quicksight Console Click me Click: Unsubscribe Cognito Userpool Go to: Cognito Console Click me\nClick Kinesis Data-Generator Users Click Delete Pool "
},
{
	"uri": "http://Nam-An-work.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://Nam-An-work.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]