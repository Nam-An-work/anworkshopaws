[
{
	"uri": "http://Nam-An-work.github.io/vi/2-ingestandstore/2.1-s3/",
	"title": "2.1 Tạo S3 bucket",
	"tags": [],
	"description": "",
	"content": "Đi đến S3 Console và tạo một bucket mới trong khu vực us-east-1:\nTruy cập vào S3 Console.\nNhấn vào - Create Bucket. Tên bucket - yourname-analytics-workshop-bucket Khu vực - US EAST (N. Virginia). Tùy chọn thêm Tags, ví dụ: workshop: AnalyticsOnAWS Nhấn vào - Create Bucket. Thêm dữ liệu tham khảo:\nMở yourname-analytics-workshop-bucket Nhấn vào - Create folder Tạo thư mục mới có tên - data Nhấn vào - Create folder Mở - data Nhấn vào - Create folder (Từ trong thư mục data) Thư mục mới: reference_data Nhấn vào - Create folder Mở - reference_data Tải tệp này xuống máy tính: tracks_list.json Trong S3 Console, nhấn vào - Upload Nhấn vào Add files và tải tệp tracks_list.json lên đây Nhấn vào Upload (góc dưới bên trái) "
},
{
	"uri": "http://Nam-An-work.github.io/vi/2-ingestandstore/2.2-kn/",
	"title": "2.2. Tạo Data Firehose",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ điều hướng đến Data Console và tạo một Data Firehose delivery stream để tiếp nhận dữ liệu và lưu trữ trong S3:\nTruy cập vào Kinesis Firehose Console. Nhấn vào - Create delivery stream. Bước 1: Choose source and destination Source: Direct PUT Destination: Amazon S3 Bước 2: Delivery stream name Delivery stream name: analytics-workshop-stream Bước 3: Transform and convert records Transform source records with AWS Lambda: Disabled (Leave \u0026lsquo;Turn on data transformation\u0026rsquo; as unchecked) Convert record format: Disabled (Leave \u0026lsquo;Enable record format conversion\u0026rsquo; as unchecked) Bước 4: Destination settings S3 bucket: naman-analytics-workshop-bucket New line delimiter: Not Enabled Dynamic partitioning: Not Enabled S3 bucket prefix: data/raw/ S3 bucket error output prefix: Leave Blank Expand Buffer hints, compression and encryption Buffer size: 1 MiB Buffer interval: 60 seconds Compression for data records: Not Enabled Encryption for data records: Use the encryption setting of the S3 bucket Bước 5: Advanced settings - Server-side encryption: unchecked - Amazon Cloudwatch error logging: Enabled - Permissions: Create or update IAM role KinesisFirehoseServiceRole-xxxx - Optionally add Tags, e.g.: workshop - AnalyticsOnAWS Bước 6: Review Lưu ý: dấu gạch chéo **/** sau **raw** là rất quan trọng. Nếu bạn bỏ qua nó, Firehose sẽ sao chép dữ liệu vào một vị trí không mong muốn. "
},
{
	"uri": "http://Nam-An-work.github.io/vi/2-ingestandstore/2.3-dummy/",
	"title": "2.3. Tạo dữ liệu giả",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ cấu hình Kinesis Data Generator để tạo dữ liệu giả và đưa nó vào Kinesis Firehose.\nCấu hình Amazon Cognito cho Kinesis Data Generator - Trong bước này, chúng ta sẽ khởi chạy một stack CloudFormation để cấu hình Cognito. Script CloudFormation này sẽ khởi chạy ở khu vực N.Virginia (Không cần thay đổi khu vực này)\nTruy cập vào AWS CloudFormation Nhấn vào - Next Cung cấp Chi tiết: Username: admin Password: choose an alphanumeric password Nhấn vào Next Các Tùy chọn: Tùy chọn thêm Tags, ví dụ: workshop: AnalyticsOnAWS Nhấn vào - Next Cuộn xuống Tôi xác nhận rằng AWS CloudFormation có thể tạo tài nguyên IAM: Check Nhấn vào Next and Submit Làm mới Console CloudFormation của bạn Đợi đến khi trạng thái stack thay đổi thành Create_Complete Chọn stack Kinesis-Data-Generator-Cognito-User Vào tab outputs: nhấn vào liên kết có tên: KinesisDataGeneratorUrl - Liên kết này sẽ mở công cụ Kinesis Data Generator Trên trang chủ Amazon Kinesis Data Generator\nĐăng nhập với tên người dùng \u0026amp; mật khẩu từ bước trước Khu vực: us-east-1 Stream/delivery stream: analytics-workshop-stream Số bản ghi mỗi giây: 2000 Mẫu bản ghi (Template 1): Trong vùng văn bản lớn, chèn mẫu json sau: { \u0026quot;uuid\u0026quot;: \u0026quot;{{random.uuid}}\u0026quot;, \u0026quot;device_ts\u0026quot;: \u0026quot;{{date.utc(\u0026quot;YYYY-MM-DD HH:mm:ss.SSS\u0026quot;)}}\u0026quot;, \u0026quot;device_id\u0026quot;: {{random.number(50)}}, \u0026quot;device_temp\u0026quot;: {{random.weightedArrayElement( {\u0026quot;weights\u0026quot;:[0.30, 0.30, 0.20, 0.20],\u0026quot;data\u0026quot;:[32, 34, 28, 40]} )}}, \u0026quot;track_id\u0026quot;: {{random.number(30)}}, \u0026quot;activity_type\u0026quot;: {{random.weightedArrayElement( { \u0026quot;weights\u0026quot;: [0.1, 0.2, 0.2, 0.3, 0.2], \u0026quot;data\u0026quot;: [\u0026quot;\\\u0026quot;Running\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Working\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Walking\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Traveling\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Sitting\\\u0026quot;\u0026quot;] } )}} } Nhấn vào - Send Data Khi công cụ gửi khoảng ~10,000 tin nhắn, bạn có thể nhấn vào - Stop sending data to Kinesis Kiểm tra xem dữ liệu đã đến S3 hay chưa\nSau một vài phút, truy cập vào S3 console Điều hướng đến: naman-analytics-workshop-bucket \u0026gt; data Sẽ có một thư mục gọi là raw \u0026gt; Mở thư mục đó và tiếp tục điều hướng, bạn sẽ nhận thấy Firehose đã đưa dữ liệu vào S3 bằng cách phân vùng theo định dạng yyyy/mm/dd/hh Nếu bạn đã nhận được dữ liệu giả trong các bucket S3 của mình, chúng ta có thể tiếp tục bước tiếp theo!\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/3-catalogdata/3.1-iam/",
	"title": "3.1. Tạo role Iam",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ truy cập vào IAM Console và tạo một vai trò dịch vụ AWS Glue mới. Vai trò này cho phép AWS Glue truy cập dữ liệu lưu trữ trong S3 và tạo các thực thể cần thiết trong Glue Data Catalog.\nĐi tới Iam Chọn Create role Chọn dịch vụ sẽ dùng tỏng Role này: Glue chọn Next Tìm AmazonS3FullAccess Chọn vào ô checkbox Tìm AWSGlueServiceRole Chọn vào ô checkbox Chọn Next Tên Role: AnalyticsworkshopGlueRole Hãy đảm bảo rằng chỉ có 2 policies attached vào role này (AmazonS3FullAccess, AWSGlueServiceRole) Tags, e.g.: workshop: AnalyticsOnAWS Chọn Create role "
},
{
	"uri": "http://Nam-An-work.github.io/vi/3-catalogdata/3.2-glue/",
	"title": "3.2. Tạo AWS Glue Crawlers",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ truy cập vào AWS Glue Console và tạo các Glue Crawlers để phát hiện schema của dữ liệu mới được nạp vào S3.\nĐi tới AWS Glue Ở bên trái tại Data Catalog, Chọn Crawlers Chọn Create crawler, Thông tin Crawler: Crawler name: AnalyticsworkshopCrawler Optionally add Tags, e.g.: workshop: AnalyticsOnAWS Chọn Next Chọn Add a data source Chọn Data source: Data source: S3 Để trống Network connection - optional Chọn In this account ở dưới Location of S3 data Include S3 path: s3://naman-analytics-workshop-bucket/data/ Ở Subsequent crawler runs để mặc định Crawl all sub-folders Chọn Add an S3 data source Chọn added S3 data source ở dưới Data Sources chọn Next IAM Role Dưới Existing IAM role, chọn AnalyticsworkshopGlueRole Để trống mọi thứ chọn Next Output configuration Chọn Add database Để xuất hiện cửa sổ mới tạo database Database details: Name: analyticsworkshopdb Click Create database Đóng cửa sổ hiện tại và quay về cửa sổ trước đó Làm mới icon bên phải Target database Chọn analyticsworkshopdb dưới Target database Ở dưới Crawler schedule Frequency: On demand Click Next Review lại các cài đặt Chọn Create crawler Bạn nên thấy thông báo: The following crawler is now created: \u0026ldquo;AnalyticsworkshopCrawler\u0026rdquo; Chọn Run crawler để crawl data lần đầu chờ vài phút Xác thực bảng đã được tạo mới trong datalog Vào Glue Catalog và khám phá data đã được crawl: VàoAWS Glue Chọn analyticsworkshopdb Chọn Tables in analyticsworkshopdb Chọn raw Khám phá cấu trúc dữ liệu Look for the averageRecordSize, recordCount, compressionType "
},
{
	"uri": "http://Nam-An-work.github.io/vi/3-catalogdata/3.3-athena/",
	"title": "3.3. Truy vấn dữ liệu đã nhập bằng Amazon Athena.",
	"tags": [],
	"description": "",
	"content": "Hãy truy vấn dữ liệu vừa được nhập vào bằng Amazon Athena.\nVào AWS Athena Nếu cần, click Edit settings trong thông báo màu xanh ở gần đầu giao diện Athena Click tab Editor Trong bảng bên trái (Database) chọn analyticsworkshopdb \u0026gt; chọn bảng raw Click vào 3 chấm (3 dấu chấm dọc) \u0026gt; Chọn Preview Table Xem lại kết quả Trong query editor, dán câu truy vấn sau: SELECT activity_type, count(activity_type) FROM raw GROUP BY activity_type ORDER BY activity_type Click Run "
},
{
	"uri": "http://Nam-An-work.github.io/vi/4-datatransformation/4.1-glue-inter/",
	"title": "4.1. Chuyển đổi dữ liệu với AWS Glue",
	"tags": [],
	"description": "",
	"content": "\nChuẩn bị chính sách IAM và vai trò\nTrong bước này, bạn sẽ truy cập vào bảng điều khiển IAM và tạo các chính sách IAM cùng vai trò cần thiết để làm việc với AWS Glue Studio Jupyter notebooks và interactive sessions. Hãy bắt đầu bằng cách tạo một chính sách IAM cho vai trò AWS Glue notebook. Truy cập vào IAM\nNhấp vào Policies trong menu bên trái\nNhấp vào Create policy\nChuyển sang tab JSON Thay thế nội dung mặc định trong trình chỉnh sửa chính sách bằng đoạn mã sau: { \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;iam:PassRole\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:iam::180294200626:role/Analyticsworkshop-GlueISRole\u0026quot; }, { \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;glue:*\u0026quot;, \u0026quot;s3:ListBucket\u0026quot;, \u0026quot;s3:GetObject\u0026quot;, \u0026quot;s3:PutObject\u0026quot;, \u0026quot;s3:DeleteObject\u0026quot;, \u0026quot;s3:GetBucketLocation\u0026quot;, \u0026quot;s3:ListAllMyBuckets\u0026quot;, \u0026quot;s3:GetLifecycleConfiguration\u0026quot;, \u0026quot;s3:GetBucketPolicy\u0026quot;, \u0026quot;s3:GetBucketCORS\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; } ] } Lưu ý rằng Analyticsworkshop-GlueISRole là vai trò mà chúng ta sẽ tạo cho AWS Glue Studio Jupyter notebook ở bước tiếp theo.\nCảnh báo: Hãy thay thế AWS account ID trong đoạn mã trên bằng ID tài khoản AWS của bạn.\nNhấp vào Next Đặt tên chính sách: AWSGlueInteractiveSessionPassRolePolicy (Tùy chọn) Viết mô tả cho chính sách: Mô tả: Chính sách này cho phép vai trò AWS Glue notebook được sử dụng trong các phiên tương tác.\n(Tùy chọn) Thêm thẻ (Tags), ví dụ: workshop: AnalyticsOnAWS\nTiếp theo, tạo một vai trò IAM cho AWS Glue notebook.\nTruy cập vào IAM Roles\nNhấp vào Roles trong menu bên trái\nNhấp vào Create Role\nChọn dịch vụ sẽ sử dụng vai trò này: Glue dưới mục \u0026ldquo;Use Case\u0026rdquo;\nNhấp vào Next Tìm kiếm và chọn các chính sách sau: - AWSGlueServiceRole - AwsGlueSessionUserRestrictedNotebookPolicy - AWSGlueInteractiveSessionPassRolePolicy - AmazonS3FullAccess Nhấp vào Next\nĐặt tên vai trò: Analyticsworkshop-GlueISRole\nĐảm bảo chỉ có bốn chính sách được gán cho vai trò này (AWSGlueServiceRole, AwsGlueSessionUserRestrictedNotebookPolicy, AWSGlueInteractiveSessionPassRolePolicy, AmazonS3FullAccess)\n(Tùy chọn) Thêm thẻ (Tags), ví dụ: workshop: AnalyticsOnAWS\nNhấp vào Create role\nLưu ý: Trong workshop này, chúng ta đã cấp quyền truy cập toàn bộ S3 cho vai trò Glue. Tuy nhiên, trong các triển khai thực tế, bạn nên cấp quyền tối thiểu cần thiết để thực hiện công việc, tuân theo nguyên tắc \u0026ldquo;least-privilege permissions model\u0026rdquo;.\n2. Sử dụng Jupyter Notebook trong AWS Glue để phát triển ETL tương tác Trong bước này, bạn sẽ tạo một AWS Glue Job với Jupyter Notebook để phát triển tập lệnh ETL trong Glue bằng PySpark. - Tải xuống và lưu file sau vào máy tính của bạn: analytics-workshop-glueis-notebook.ipynb - Truy cập: Glue Studio jobs - Chọn Jupyter Notebook - Chọn Upload and edit an existing notebook - Nhấn Choose file - Tải lên analytics-workshop-glueis-notebook.ipynb vừa tải xuống - Nhấn Create - Trong phần Cấu hình Notebook: - Tên Job: AnalyticsOnAWS-GlueIS - IAM Role: Analyticsworkshop-GlueISRole - Kernel để mặc định là Spark - Nhấn Start notebook Sau khi notebook khởi động, hãy làm theo hướng dẫn bên trong notebook.\nXác minh - Dữ liệu đã được xử lý có trong S3 Sau khi tập lệnh ETL chạy thành công, quay lại giao diện điều khiển S3: Nhấn vào đây Mở thư mục naman-analytics-workshop-bucket \u0026gt; data Truy cập vào thư mục processed-data: Kiểm tra xem các file .parquet đã được tạo chưa. Bây giờ dữ liệu đã được xử lý, bạn có thể truy vấn bằng Amazon Athena hoặc tiếp tục xử lý/tổng hợp với AWS Glue hoặc Amazon EMR. Bước tiếp theo về EMR là tùy chọn, bạn có thể bỏ qua và tiếp tục đến Phân tích với Athena. "
},
{
	"uri": "http://Nam-An-work.github.io/vi/4-datatransformation/4.2-glue-grap/",
	"title": "4.2. Chuyển đổi dữ liệu với AWS Glue Studio (graphical interface)",
	"tags": [],
	"description": "",
	"content": "\nAWS Glue Studio là gì? AWS Glue Studio là giao diện đồ họa mới giúp bạn dễ dàng tạo, chạy và giám sát các tác vụ extract, transform, và load (ETL) trong AWS Glue. Bạn có thể trực quan soạn thảo quy trình chuyển đổi dữ liệu và chạy chúng liền mạch trên động cơ ETL serverless dựa trên Apache Spark của AWS Glue.\nTrong bài thực hành này, chúng ta sẽ thực hiện quy trình ETL tương tự như \u0026ldquo;Chuyển đổi Dữ liệu với AWS Glue (phiên tương tác)\u0026rdquo;.\nNhưng lần này, chúng ta sẽ tận dụng giao diện đồ họa trực quan trong AWS Glue Studio!\nThực hành Truy cập vào Glue Studio Console Nhấp vào Jobs và chọn Visual với blank canvas Nhấp vào Create Nhấp vào Source và chọn S3 Nhấp vào tab Data source properties - S3 trong cửa sổ cấu hình bên phải màn hình. Dưới S3 source type chọn Data Catalog table Database - analyticsworkshopdb Table - raw Bây giờ, hãy lặp lại các bước tương tự để thêm reference_data từ S3. Nhấp vào Source và chọn S3 Nhấp vào tab Data source properties - S3 trong cửa sổ cấu hình bên phải màn hình. Dưới S3 source type chọn Data Catalog table Database - analyticsworkshopdb Table - reference_data Nhấp vào node S3 đã được thêm trước đó trên canvas, sau đó nhấp vào add node và chọn Change Schema. Chọn tab Transform bên phải và thay đổi Data type của track_id thành int. Nhấp vào node S3 bên trái trên canvas, sau đó nhấp vào add node và chọn Join Bạn sẽ thấy sơ đồ trực quan như trong ảnh chụp màn hình bên dưới, và thông báo bên phải \u0026ldquo;Insufficient source nodes\u0026rdquo; vì bạn cần thêm một node (Data source) để thực hiện phép join. Tiếp theo, nhấp vào node Transform - Join và trong cửa sổ cấu hình bên phải, chọn dropdown và chọn Change Schema dưới mục Transforms như trong ảnh dưới đây: Nhấp vào Add condition. Chọn cột track_id làm cột join như hiển thị trong ảnh dưới đây. Với node Join được chọn trên canvas, nhấp vào add node và chọn Change Schema Bạn sẽ thấy sơ đồ trực quan như trong ảnh chụp màn hình bên dưới. Chúng ta sẽ loại bỏ các cột không sử dụng và ánh xạ kiểu dữ liệu mới cho các cột sau: Loại bỏ các cột: parition_0 parition_1 parition_2 parition_3 Ánh xạ kiểu dữ liệu mới: track_id: string Nhấp vào node Transform - ApplyMapping trên canvas. Nhấp vào Target và chọn S3 như hiển thị trong ảnh dưới đây. Trong Data target properties - S3, nhập các thông số sau: Format: Parquet Compression Type: Snappy S3 Target Location: s3://naman-analytics-workshop-bucket/data/processed-data2/ Data Catalog update options: Chọn \u0026ldquo;Create a table in the Data Catalog\u0026rdquo; và trong các lần chạy sau, cập nhật schema và thêm partition mới. Database: analyticsworkshopdb Table name: processed-data2 Nhấp vào Job details và cấu hình với các tùy chọn sau: Name: AnalyticsOnAWS-GlueStudio IAM Role: AnalyticsWorkshopGlueRole Requested number of workers: 2 Job bookmark: Disable Number of retries: 1 Job timeout (minutes): 10 Giữ các giá trị mặc định còn lại. Nhấp vào Save Nhấp vào Save và bạn sẽ thấy thông báo \u0026ldquo;Successfully created job\u0026rdquo;. Bắt đầu tác vụ ETL này bằng cách nhấp vào Run ở góc trên bên phải màn hình. Chờ vài giây và bạn sẽ thấy trạng thái chạy của ETL job là \u0026ldquo;Succeeded\u0026rdquo; như trong ảnh chụp màn hình bên dưới. Bạn có thể xem mã Pyspark mà Glue Studio đã tạo ra và tái sử dụng mã này cho các mục đích khác nếu cần. Truy cập vào Glue DataCatalog: Nhấn vào đây và bạn sẽ thấy bảng processed-data2 được tạo dưới database analyticsworkshopdb. Well Done!! Bạn đã hoàn thành bài thực hành ETL bổ sung với AWS Glue Studio. Với AWS Glue Studio, bạn có thể trực quan soạn thảo quy trình chuyển đổi dữ liệu và chạy liền mạch trên động cơ ETL serverless dựa trên Apache Spark của AWS Glue. "
},
{
	"uri": "http://Nam-An-work.github.io/vi/4-datatransformation/4.3-glue-data/",
	"title": "4.3. Chuyển đổi dữ liệu với AWS Glue DataBrew",
	"tags": [],
	"description": "",
	"content": "\nAWS Glue DataBrew là gì? AWS Glue DataBrew là một công cụ trực quan mới giúp các nhà phân tích dữ liệu và nhà khoa học dữ liệu dễ dàng làm sạch và chuẩn hóa dữ liệu để chuẩn bị cho phân tích và máy học. Bạn có thể chọn từ hơn 250 phép biến đổi có sẵn để tự động hóa các tác vụ chuẩn bị dữ liệu mà không cần viết bất kỳ dòng mã nào.\nDataBrew giúp bạn tự động lọc các giá trị bất thường, chuyển đổi dữ liệu sang định dạng tiêu chuẩn, sửa các giá trị không hợp lệ và thực hiện nhiều tác vụ khác. Sau khi dữ liệu sẵn sàng, bạn có thể sử dụng ngay cho các dự án phân tích và máy học. Bạn chỉ trả tiền cho những gì bạn sử dụng, không có cam kết trả trước.\nThực hành Truy cập Glue DataBrew Console\nNhấn Create project Nhập Project name: AnalyticsOnAWS-GlueDataBrew Chọn New dataset Nhập raw-dataset làm Dataset name Chọn All AWS Glue tables để kết nối với Glue Catalog Chọn analyticsworkshopdb và bảng raw Trong Permissions, chọn Create new IAM role và đặt tên AnalyticsOnAWS-GlueDataBrew Nhấn Create project Khám phá dữ liệu trong Glue DataBrew\nKhi phiên DataBrew được tạo, bạn sẽ thấy giao diện như hình dưới: Nhấn SCHEMA để xem thông tin cột, kiểu dữ liệu và phân bố dữ liệu Quay lại GRID và thay đổi kiểu dữ liệu của track_id thành string Chạy profiling để phân tích dữ liệu\nNhấn PROFILE và chọn Run data profile Giữ nguyên Job name và Job run sample Đặt vị trí lưu trên S3: s3://naman-analytics-workshop-bucket/ Chọn IAM role đã tạo và nhấn Create and run job Bạn sẽ thấy trạng thái job như sau: Kết hợp dữ liệu từ nhiều bảng\nTrở lại GRID, nhấn Join Chọn Connect new dataset Chọn All AWS Glue tables → analyticsworkshopdb → reference_data Nhập reference-data-dataset làm tên dataset Nhấn Next, chọn: track_id từ raw-dataset track_id từ reference-data-dataset Bỏ chọn track_id từ bảng B Nhấn Finish Kết quả sẽ như sau: Xem thông tin chi tiết dữ liệu\nNhấn PROFILE để xem thông tin như dữ liệu bị thiếu, giá trị trùng lặp, phân bố giá trị,\u0026hellip; Nhấn LINEAGE để xem trực quan luồng dữ liệu từ nguồn đến đầu ra Tạo và chạy job xử lý dữ liệu\nQuay lại GRID, nhấn Create job Cấu hình: Job name: AnalyticsOnAWS-GlueDataBrew-Job File type: GlueParquet S3 location: s3://naman-analytics-workshop-bucket/data/processed-data/ Chọn IAM role đã tạo, nhấn Create and run job Bạn sẽ thấy 1 job đang chạy: Trong Jobs, nhấn vào Job name để xem chi tiết lịch sử chạy Khi hoàn tất (mất 4-5 phút), bạn sẽ thấy trạng thái Succeeded Nhấn 1 output dưới cột Output Nhấn vào đường dẫn S3 để xem file đầu ra Bạn sẽ thấy file kết quả từ Glue DataBrew! Chúc mừng! Bạn đã hoàn thành bài thực hành với AWS Glue DataBrew, giúp làm sạch và chuẩn bị dữ liệu nhanh chóng, trực quan mà không cần viết mã.\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/4-datatransformation/4.4-emr/",
	"title": "4.4. Chuyển đổi dữ liệu với EMR",
	"tags": [],
	"description": "",
	"content": "\n1. Tải script lên S3 Trong bước này, chúng ta sẽ tạo các thư mục trên S3 để sử dụng trong bước chạy EMR.\nTruy cập: S3 Console Thêm script PySpark: Mở naman-analytics-workshop-bucket Nhấn Create folder → đặt tên scripts → Nhấn Create folder Mở scripts Tải xuống file này Trong S3 console, nhấn Upload Tạo thư mục lưu log EMR: Mở naman-analytics-workshop-bucket Nhấn Create folder → đặt tên logs → Nhấn Create folder 2. Tạo EMR cluster và thêm bước xử lý Truy cập EMR console Nhấn Create cluster Cấu hình cluster: Cluster name: analytics-workshop-transformer Amazon EMR release: mặc định (ví dụ: emr-6.10.0) Application bundle: Spark AWS Glue Data Catalog settings: bỏ chọn Use for Spark table metadata Giữ nguyên các cài đặt khác Cấu hình tài nguyên: Instance groups: Giữ nguyên mặc định (m5.xlarge) Cluster scaling: Giữ nguyên (Core: 1, Task -1: 1) Giữ nguyên phần Networking Thêm bước xử lý: Nhấn Add Step Type: Spark application Name: Spark job Deploy mode: Cluster mode Application location: s3://naman-analytics-workshop-bucket/scripts/emr_pyspark.py Arguments: nhập tên bucket S3 naman-analytics-workshop-bucket Action if step fails: Terminate cluster Nhấn Save step Cấu hình tự động tắt cluster: Idle time: 0 days 01:00:00 Chọn Terminate cluster after last step completes Bỏ chọn Use termination protection Cấu hình log: Chọn Publish cluster-specific logs to Amazon S3 Nhập S3 location: s3://naman-analytics-workshop-bucket/logs/ IAM roles: Chọn Create a service role cho Amazon EMR Chọn Create an instance profile cho EC2 instance profile S3 bucket access: All S3 buckets in this account with read and write access Nhấn Create cluster 3. Kiểm tra trạng thái job EMR EMR cluster sẽ mất khoảng 6-8 phút để khởi tạo và 1 phút để chạy Spark job. Sau khi hoàn thành, cluster sẽ tự động tắt. Để kiểm tra trạng thái job: Truy cập Cluster name: analytics-workshop-transformer Chuyển đến tab Steps Bạn sẽ thấy Spark application và Setup hadoop debugging Trạng thái Spark application sẽ thay đổi từ Pending → Running → Completed Sau khi hoàn tất, trạng thái EMR Cluster sẽ là Terminated với thông báo All steps completed. 4. Xác nhận dữ liệu đã xử lý trên S3 Truy cập S3 console Kiểm tra dữ liệu đầu ra: Mở naman-analytics-workshop-bucket \u0026gt; data Mở thư mục emr-processed-data Kiểm tra file .parquet đã được tạo trong thư mục này. 5. Chạy lại Glue Crawler Truy cập: Glue console Trong menu bên trái, nhấn Crawlers Chọn AnalyticsworkshopCrawler Nhấn Run Trạng thái sẽ thay đổi từ Starting → Running Sau vài phút, crawler sẽ hoàn thành với Tables added = 1 Kiểm tra trong mục Databases để xác nhận bảng emr_processed_data đã được thêm vào. Bây giờ, bạn có thể sử dụng Amazon Athena để truy vấn kết quả từ job EMR. "
},
{
	"uri": "http://Nam-An-work.github.io/vi/6-analyzewithkinesis/6.1-iam/",
	"title": "6.1. Tạo Role Iam",
	"tags": [],
	"description": "",
	"content": " Truy cập vào AWS IAM Role Nhấn Create role (Tạo vai trò) Chọn dịch vụ Kinesis trong phần Use case từ menu thả xuống có dòng chữ Use cases for other AWS services: Chọn Kinesis Analytics Nhấn Next để tiếp tục với phần Permissions Tìm kiếm AWSGlueServiceRole và chọn ô checkbox tương ứng Tìm kiếm AmazonKinesisFullAccess và chọn ô checkbox tương ứng Nhấn Next Nhập tên Vai trò: AnalyticsworkshopKinesisAnalyticsRole Đảm bảo rằng chỉ có hai chính sách được gắn với vai trò này (AWSGlueServiceRole, AmazonKinesisFullAccess) Tùy chọn thêm Tags, ví dụ: workshop: AnalyticsOnAWS Nhấn Create Role (Tạo Vai trò) "
},
{
	"uri": "http://Nam-An-work.github.io/vi/6-analyzewithkinesis/6.2-kin/",
	"title": "6.2. Tạo Kinesis Data Stream",
	"tags": [],
	"description": "",
	"content": "Kinesis Data Generator là một ứng dụng giúp bạn dễ dàng gửi dữ liệu thử nghiệm đến luồng dữ liệu Amazon Kinesis hoặc luồng phân phối Amazon Kinesis Firehose. Chúng ta sẽ tạo một Kinesis Data Stream để tiếp nhận dữ liệu từ Kinesis Data Generator. Notebook Kinesis Application của chúng ta sẽ đọc dữ liệu luồng từ Kinesis Data Stream này.\nTruy cập vào Dịch vụ AWS Kinesis Nhấn Create Data Stream (Tạo Luồng Dữ Liệu) Nhập tên luồng dữ liệu: analytics-workshop-data-stream Trong phần Capacity của Data Stream: Chọn Capacity Mode: Provisioned Cài đặt Provisioned Shards: 2 Cuộn xuống dưới cùng và nhấn Create Data Stream (Tạo Luồng Dữ Liệu) "
},
{
	"uri": "http://Nam-An-work.github.io/vi/6-analyzewithkinesis/6.3-glue/",
	"title": "6.3. Tạo bảng Glue Catalog",
	"tags": [],
	"description": "",
	"content": "Notebook Kinesis Application của chúng ta sẽ lấy thông tin về nguồn dữ liệu từ AWS Glue. Khi bạn tạo notebook Studio, bạn chỉ định cơ sở dữ liệu AWS Glue chứa thông tin kết nối của bạn. Khi truy cập vào các nguồn dữ liệu, bạn chỉ định các bảng AWS Glue có trong cơ sở dữ liệu.\nTruy cập vào AWS Glue Từ thanh bên trái, vào phần Databases và nhấn vào cơ sở dữ liệu chúng ta đã tạo trước đó analyticsworkshopdb Nhấn Tables in analyticsworkshopdb Nhấn menu thả xuống Add tables và chọn Add table manually \\ Trong Table Properties\nNhập tên bảng: raw_stream Trong Data store\nChọn loại nguồn dữ liệu: Kinesis Bỏ qua phần Select a kinesis data stream (Luồng trong tài khoản của tôi sẽ được chọn mặc định) Khu vực: US East (N. Virginia) us-east-1 Tên luồng Kinesis: analytics-workshop-data-stream Bỏ qua phần kích thước mẫu Trong Data format\nClassification: JSON Sau đó nhấn Next Trong Schema\nNhấn Edit Schema as JSON và chèn đoạn văn bản JSON sau: [{ \u0026quot;Name\u0026quot;: \u0026quot;uuid\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_ts\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;timestamp\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_id\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;device_temp\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;track_id\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;int\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }, { \u0026quot;Name\u0026quot;: \u0026quot;activity_type\u0026quot;, \u0026quot;Type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;Comment\u0026quot;: \u0026quot;\u0026quot; }] Sau đó nhấn Next Bỏ qua Partition Indices Review and create\nXác minh rằng bảng Glue mới raw_stream đã được tạo chính xác. Làm mới danh sách bảng nếu bảng chưa xuất hiện. Nhấn vào bảng mới tạo raw_stream Nhấn Actions và nhấn Edit table Trong phần Table properties, thêm thuộc tính mới: Key: kinesisanalytics.proctime Value: proctime "
},
{
	"uri": "http://Nam-An-work.github.io/vi/6-analyzewithkinesis/6.4-stu/",
	"title": "6.4. Tạo Sổ tay Ứng dụng Phân tích Dữ liệu Luồng trong Studio",
	"tags": [],
	"description": "",
	"content": "Tạo Analytics Streaming Application Studio Notebook Bây giờ chúng ta hãy tạo Kinesis Analytics Streaming Application Studio Notebook. Notebook ứng dụng phân tích này có thể xử lý dữ liệu luồng từ Kinesis Data Stream và chúng ta có thể viết các truy vấn SQL để có được những cái nhìn thời gian thực như số lượng hoạt động hiện tại hoặc nhiệt độ thiết bị.\nTruy cập vào Managed Apache Flink Vào phần Studio Nhấn Create Studio notebook (Tạo notebook Studio) Chọn Create with custom settings (Tạo với cài đặt tùy chỉnh) General\nNhập tên Studio notebook: AnalyticsWorkshop-KDANotebook Nhập Runtime: Apache Flink 1.15, Apache Zeppelin 0.10 Nhấn Next IAM role\nChọn Choose from IAM roles that Kinesis Data Analytics can assume (Chọn từ các vai trò IAM mà Kinesis Data Analytics có thể giả định) Chọn vai trò dịch vụ đã tạo trước đó: AnalyticsworkshopKinesisAnalyticsRole Trong cơ sở dữ liệu AWS Glue, chọn: analyticsworkshopdb Nhấn Next Trong Configurations\nParallelism: 4 Parallelism per KPU: 1 Bỏ chọn ô Turn on logging Bỏ qua các cài đặt khác và đi xuống dưới cùng, trong tag: workshop: AnalyticsOnAWS Chạy Kinesis Analytics Studio Notebook Bây giờ chúng ta đã tạo notebook, chúng ta có thể chạy nó và thử thực hiện một số truy vấn SQL.\nXem danh sách notebook trong tab Studio và nhấn vào notebook vừa tạo AnalyticsWorkshop-KDANotebook Nhấn Run Chờ đến khi trạng thái chuyển sang chế độ Running. (Mất khoảng 5-7 phút) Nhấn Open in Apache Zeppelin ở góc trên bên phải để mở Zeppelin Notebook Nhấn Create new note và đặt tên cho note là AnalyticsWorkshop-ZeppelinNote Dán truy vấn SQL này %flink.ssql(type=update) SELECT * FROM raw_stream; Nhấn Add Paragraph Dán truy vấn SQL này %flink.ssql(type=update) SELECT activity_type, count(*) as activity_cnt FROM raw_stream group by activity_type; Khi tất cả các truy vấn đã được dán, nhấn nút Play ở góc trên bên phải của đoạn văn LƯU Ý: Sẽ chưa có dữ liệu hiển thị vì notebook Analytics Streaming này xử lý dữ liệu luồng. Chúng ta phải gửi dữ liệu luồng từ Kinesis Data Generator (nếu chưa làm), để notebook có thể hiển thị kết quả của các truy vấn.\nTạo Dữ Liệu Giả cho Kinesis Data Stream Để hiển thị dữ liệu từ các truy vấn trong Analytics Streaming notebook, chúng ta phải gửi dữ liệu thô từ Kinesis Data Generator.\nTruy cập vào KinesisDataGeneratorURL. Bạn có thể tìm thấy điều này trong tab Output của Cloudformation stack. Đăng nhập với tên người dùng và mật khẩu của bạn Điền thông tin sau:\nKhu vực: us-east-1\nStream/delivery stream: analytics-workshop-data-stream (KHÔNG chọn analytics-workshop-stream mà bạn có thể đã tạo trong module \u0026ldquo;Ingest and Store\u0026rdquo; của khóa học này)\nĐảm bảo Records per second là Constant.\nGiá trị cho Records per second: 100 (KHÔNG thay đổi số này cho khóa học.)\nĐảm bảo Compress Records không được chọn.\nTemplate Record (Template 1): Trong khu vực văn bản lớn, chèn mẫu JSON sau: { \u0026quot;uuid\u0026quot;: \u0026quot;{{random.uuid}}\u0026quot;, \u0026quot;device_ts\u0026quot;: \u0026quot;{{date.utc(\u0026quot;YYYY-MM-DD HH:mm:ss.SSS\u0026quot;)}}\u0026quot;, \u0026quot;device_id\u0026quot;: {{random.number(50)}}, \u0026quot;device_temp\u0026quot;: {{random.weightedArrayElement( {\u0026quot;weights\u0026quot;:[0.30, 0.30, 0.20, 0.20],\u0026quot;data\u0026quot;:[32, 34, 28, 40]} )}}, \u0026quot;track_id\u0026quot;: {{random.number(30)}}, \u0026quot;activity_type\u0026quot;: {{random.weightedArrayElement( { \u0026quot;weights\u0026quot;: [0.1, 0.2, 0.2, 0.3, 0.2], \u0026quot;data\u0026quot;: [\u0026quot;\\\u0026quot;Running\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Working\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Walking\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Traveling\\\u0026quot;\u0026quot;, \u0026quot;\\\u0026quot;Sitting\\\u0026quot;\u0026quot;] } )}} } Sau đó nhấn Send Data Quay lại Zeppelin Notebook của chúng ta. Chờ một vài phút, kết quả sẽ hiển thị "
},
{
	"uri": "http://Nam-An-work.github.io/vi/1-introduce/",
	"title": "1. Giới thiệu",
	"tags": [],
	"description": "",
	"content": "Kết quả đạt được từ kiến trúc này Thiết kế và xây dựng kiến trúc Data Lake Thiết kế kiến trúc data lake không máy chủ Xây dựng pipeline xử lý dữ liệu và Data Lake sử dụng Amazon S3 để lưu trữ dữ liệu Truyền phát và phân tích dữ liệu thời gian thực Truyền phát dữ liệu thời gian thực bằng Amazon Kinesis Phân tích dữ liệu thời gian thực với Amazon Kinesis Data Analytics Quản lý và xử lý dữ liệu Tự động lập chỉ mục các bộ dữ liệu bằng AWS Glue Chuyển đổi và xử lý dữ liệu hiệu quả Chạy các script ETL tương tác trong Jupyter notebook trên AWS Glue Studio qua các phiên tương tác của AWS Glue Quản lý và giám sát công việc ETL Giám sát và quản lý các công việc ETL thông qua Glue Studio Chuẩn bị và biến đổi dữ liệu Chuẩn bị dữ liệu dễ dàng với Glue DataBrew Chạy các công việc Spark transformation trên EMR Tải và tối ưu hóa dữ liệu trong kho dữ liệu Tải dữ liệu từ Glue vào Amazon Redshift Áp dụng các nguyên tắc thiết kế tối ưu cho Amazon Redshift Truy vấn và trực quan hóa dữ liệu Truy vấn và phân tích dữ liệu qua Amazon Athena Trực quan hóa kết quả bằng Amazon QuickSight Điều kiện tiên quyết Bạn cần có quyền truy cập vào tài khoản AWS với quyền AdministratorAccess Bài lab này nên được thực hiện trong khu vực us-east-1 "
},
{
	"uri": "http://Nam-An-work.github.io/vi/2-ingestandstore/",
	"title": "2. Tiếp nhận và lưu trữ dữ liệu",
	"tags": [],
	"description": "",
	"content": " Để bắt đầu, chúng ta sẽ tạo ra một số dữ liệu mẫu trong thời gian gần như thực bằng công cụ Kinesis Data Generator và chuyển dữ liệu đến Amazon S3 qua dòng dữ liệu Kinesis Firehose.\nNgoài ra, chúng ta sẽ tải trực tiếp một số dữ liệu tham chiếu vào bucket Amazon S3.\nĐể bắt đầu, đăng nhập AWS Console tại region us-east-1: Click me\nNội dung 2.1. Tạo S3 Bucket 2.2. Tạo Kinesis Firehose 2.3. Tạo dữ liệu mẫu\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/3-catalogdata/",
	"title": "3. Dữ liệu mô tả",
	"tags": [],
	"description": "",
	"content": " Tiếp theo, chúng ta sẽ đăng ký các bộ dữ liệu trong AWS Glue Data Catalog và tự động hóa việc thu thập siêu dữ liệu bằng cách sử dụng Glue Crawlers.\nSau khi các thực thể trong catalog được tạo, chúng ta có thể bắt đầu truy vấn dữ liệu ở định dạng thô bằng Amazon Athena\nNội dung 3.1. Tạo role Iam 3.2. Tạo AWS Glue Crawlers 3.3. Truy vấn dữ liệu đã nhập bằng Amazon Athena\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/4-datatransformation/",
	"title": "4. Chuyển đổi dữ liệu",
	"tags": [],
	"description": "",
	"content": "Bây giờ chúng ta đã ghi lại dữ liệu, hãy tiếp tục với bước tiếp theo là chuyển đổi dữ liệu bằng AWS Glue ETL!\nNội dung Content 4.1. Chuyển đổi dữ liệu với AWS Glue 4.2. Chuyển đổi dữ liệu với AWS Glue Studio 4.3. Chuyển đổi dữ liệu với AWS Glue DataBrew 4.4. Chuyển đổi dữ liệu với EMR\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/5-analyzewithathena/",
	"title": "5. Phân tích với Athena",
	"tags": [],
	"description": "",
	"content": " Trong bước này, chúng ta sẽ thực hiện tạo kết nối đến các máy chủ EC2 của chúng ta, nằm trong cả public và private subnet.\n1. Đăng nhập vào Amazon Athena Console Truy cập: Athena Console Athena sử dụng AWS Glue Catalog để quản lý nguồn dữ liệu, vì vậy bất kỳ bảng nào được hỗ trợ bởi S3 trong Glue đều có thể được truy vấn qua Athena. 2. Chạy truy vấn phân tích dữ liệu Trong bảng điều khiển bên trái, chọn analyticsworkshopdb từ menu dropdown. Khám phá giao diện Athena và thử chạy một số truy vấn. Hãy thử truy vấn bảng emr_processed_data. SELECT artist_name, count(artist_name) AS count FROM processed_data GROUP BY artist_name ORDER BY count desc Truy vấn này trả về danh sách các bài hát được phát lại nhiều lần trên các thiết bị. SELECT device_id, track_name, count(track_name) AS count FROM processed_data GROUP BY device_id, track_name ORDER BY count desc Xem bảng "
},
{
	"uri": "http://Nam-An-work.github.io/vi/6-analyzewithkinesis/",
	"title": "6. Phân tích với Kinesis Data Analytics",
	"tags": [],
	"description": "",
	"content": " Trong phần trước, bạn đã khám phá cách phân tích dữ liệu bằng Amazon Athena. Trong phần này, chúng ta sẽ tìm hiểu cách thực hiện phân tích dữ liệu theo thời gian thực từ luồng dữ liệu bằng Amazon Kinesis Data Analytics. Điều này có thể được thực hiện theo 2 cách: sử dụng Ứng dụng SQL cũ hoặc sử dụng Studio notebook mới được khuyến nghị. Trong khóa học này, chúng ta sẽ sử dụng Studio notebook và tạo Ứng dụng Kinesis Analytics dựa trên SQL.\nNội dung 6.1. Tạo Role Iam 6.2. Tạo Kinesis Data Stream\n6.3. Tạo bảng Glue Catalog\n6.4. Tạo Sổ tay Ứng dụng Phân tích Dữ liệu Luồng trong Studio\n"
},
{
	"uri": "http://Nam-An-work.github.io/vi/7-quicksight/",
	"title": "7. Trực quan hóa trong Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": " Trong mô-đun này, chúng ta sẽ sử dụng Amazon Quicksight để xây dựng một số hình ảnh trực quan về dữ liệu được thu thập và lưu trữ trong S3.\nCài Đặt QuickSight Trong bước này, chúng ta sẽ trực quan hóa dữ liệu đã xử lý bằng QuickSight.\nTruy cập vào: Quicksight Console Nhấn Sign up for QuickSight (Đăng ký QuickSight) Đảm bảo chọn Enterprise và nhấn Continue (Tiếp tục) Tên tài khoản QuickSight: yournameanalyticsworkshop Địa chỉ email thông báo: you@youremail.com Chọn Amazon Athena - điều này cho phép QuickSight truy cập vào cơ sở dữ liệu Amazon Athena Chọn Amazon S3 Chọn naman-analytics-workshop-bucket Nhấn Finish Chờ cho tài khoản QuickSight của bạn được tạo Thêm Dữ Liệu Mới Truy cập vào quicksight Ở góc trên bên phải, nhấn New Analyze (Phân tích Mới) Ở góc trên bên trái, nhấn New Dataset (Dữ liệu Mới) Nhấn Athena Nguồn dữ liệu Athena mới Tên nguồn dữ liệu: analyticsworkshop Nhấn Validate connection (Xác nhận kết nối) Nhấn Create data source (Tạo nguồn dữ liệu) Chọn bảng của bạn\nCơ sở dữ liệu: chứa các bộ bảng - chọn analyticsworkshopdb Bảng: chứa dữ liệu bạn có thể trực quan hóa - chọn processed_data Nhấn Select (Chọn) Hoàn thành tạo bộ dữ liệu Chọn Directly query your data (Truy vấn trực tiếp dữ liệu của bạn) Nhấn Visualize (Trực quan hóa) Sử Dụng Amazon QuickSight để Trực Quan Hóa Dữ Liệu Đã Xử Lý Ở bảng phía dưới bên trái (Các loại hình trực quan):\nDi chuột qua các biểu tượng để xem tên các hình trực quan có sẵn Nhấn vào Heat Map (Bản đồ nhiệt) Ở bảng phía trên bên trái (Danh sách các trường): Nhấn device_id Nhấn track_name Ngay trên hình trực quan, bạn sẽ thấy Field wells: Dòng: device_id | Cột: track_name "
},
{
	"uri": "http://Nam-An-work.github.io/vi/8-servewithlambda/",
	"title": "8. Cung cấp với Lambda",
	"tags": [],
	"description": "",
	"content": " Trong bước này, chúng ta sẽ thực hiện tạo kết nối đến các máy chủ EC2 của chúng ta, nằm trong cả public và private subnet. Tạo Hàm Lambda Truy cập vào: Lambda Console\nNhấn Create function (Tạo hàm)\nChọn Author from scratch (Tạo từ đầu) Dưới phần Thông tin cơ bản\nĐặt tên hàm là Analyticsworkshop_top5Songs\nChọn Runtime là Python 3.9\nMở rộng phần Choose or create an execution role dưới phần Permissions, đảm bảo chọn Create a new role with basic Lambda permissions. Cuộn xuống phần Function Code và thay thế mã hiện tại trong lambda_function.py bằng đoạn mã Python dưới đây: import boto3 import time import os # Các biến môi trường DATABASE = os.environ['DATABASE'] TABLE = os.environ['TABLE'] # Hằng số Top X TOPX = 5 # Hằng số S3 S3_OUTPUT = f's3://{os.environ[\u0026quot;BUCKET_NAME\u0026quot;]}/query_results/' # Số lần thử lại RETRY_COUNT = 10 def lambda_handler(event, context): client = boto3.client('athena') # biến truy vấn với hai biến môi trường và một hằng số query = f\u0026quot;\u0026quot;\u0026quot; SELECT track_name as \\\u0026quot;Track Name\\\u0026quot;, artist_name as \\\u0026quot;Artist Name\\\u0026quot;, count(1) as \\\u0026quot;Hits\\\u0026quot; FROM {DATABASE}.{TABLE} GROUP BY 1,2 ORDER BY 3 DESC LIMIT {TOPX}; \u0026quot;\u0026quot;\u0026quot; response = client.start_query_execution( QueryString=query, QueryExecutionContext={ 'Database': DATABASE }, ResultConfiguration={'OutputLocation': S3_OUTPUT} ) query_execution_id = response['QueryExecutionId'] # Kiểm tra trạng thái thực thi for i in range(0, RETRY_COUNT): # Lấy Trạng thái thực thi truy vấn query_status = client.get_query_execution( QueryExecutionId=query_execution_id ) exec_status = query_status['QueryExecution']['Status']['State'] if exec_status == 'SUCCEEDED': print(f'Status: {exec_status}') break elif exec_status == 'FAILED': raise Exception(f'STATUS: {exec_status}') else: print(f'STATUS: {exec_status}') time.sleep(i) else: client.stop_query_execution(QueryExecutionId=query_execution_id) raise Exception('TIME OVER') # Lấy kết quả truy vấn result = client.get_query_results(QueryExecutionId=query_execution_id) print(result['ResultSet']['Rows']) # Hàm có thể trả kết quả cho ứng dụng hoặc dịch vụ của bạn # return result['ResultSet']['Rows'] Biến Môi Trường Cuộn xuống phần Environment variables và thêm ba biến môi trường dưới đây.\nKey: DATABASE, Value: analyticsworkshopdb\nKey: TABLE, Value: processed_data\nKey: BUCKET_NAME, Value: yourname-analytics-workshop-bucket Để mặc định Memory (MB) là 128 MB\nThay đổi Timeout thành 10 giây. Quyền Thực Thi Chọn tab Permissions ở trên cùng: Nhấn vào liên kết Role Name dưới Execution Role để mở IAM Console trong một tab mới. Nhấn Add permissions và nhấn Attach policies Thêm hai chính sách sau (tìm kiếm trong hộp lọc, đánh dấu và nhấn Attach policy): AmazonS3FullAccess AmazonAthenaFullAccess Sau khi các chính sách này được đính kèm vào vai trò, đóng tab này. Cấu Hình Sự Kiện Kiểm Tra Hàm của chúng ta đã sẵn sàng để thử nghiệm. Triển khai hàm trước bằng cách nhấn Deploy dưới phần mã hàm.\nTiếp theo, hãy cấu hình sự kiện thử nghiệm giả để xem kết quả thực thi của hàm lambda mới tạo.\nNhấn Test ở góc trên bên phải của bảng điều khiển lambda.\nMột cửa sổ mới sẽ bật lên để chúng ta cấu hình sự kiện thử nghiệm. Create new test event (Tạo sự kiện thử mới) được chọn mặc định. Tên sự kiện: Test Template: Hello World Để mọi thứ mặc định Nhấn Save Nhấn Test lần nữa Bạn sẽ thấy kết quả đầu ra ở định dạng JSON trong phần Execution Result. Xác Minh Qua Athena Truy cập vào: Athena Console Ở panel bên trái, chọn analyticsworkshopdb từ danh sách thả xuống Chạy truy vấn sau SELECT track_name as \u0026quot;Track Name\u0026quot;, artist_name as \u0026quot;Artist Name\u0026quot;, count(1) as \u0026quot;Hits\u0026quot; FROM analyticsworkshopdb.processed_data GROUP BY 1,2 ORDER BY 3 DESC LIMIT 5; "
},
{
	"uri": "http://Nam-An-work.github.io/vi/9-warehouseonredshift/",
	"title": "9. Kho dữ liệu trên Redshift",
	"tags": [],
	"description": "",
	"content": " Trong mô-đun này, chúng ta sẽ thiết lập cụm Amazon Redshift và sử dụng AWS Glue để tải dữ liệu vào Amazon Redshift. Chúng ta sẽ tìm hiểu về một số cân nhắc về thiết kế và các phương pháp hay nhất về việc tạo và tải dữ liệu vào các bảng trong Redshift và chạy các truy vấn trên đó.\nTạo IAM Role cho Redshift Truy cập vào: Redshift Console\nNhấn vào Create role (Tạo vai trò) Chọn Redshift dưới Use case và Use cases for other AWS services: Chọn Redshift - customizable Nhấn Next Trong hộp tìm kiếm, tìm và chọn hai chính sách sau\nAmazonS3FullAccess\nAWSGlueConsoleFullAccess\nNhấn Next\nĐặt tên Role là Analyticsworkshop_RedshiftRole Tùy chọn thêm Tags, ví dụ: workshop: AnalyticsOnAWS\nNhấn Create role (Tạo vai trò) Tạo Cụm Redshift Truy cập vào Redshift cluster\nNhấn vào Provision và quản lý các cụm từ bảng điều khiển bên trái\nNhấn vào Create Cluster (Tạo Cụm)\nĐể Cluster identifier là redshift-cluster-1\nChọn dc2.large làm Node Type\nChọn Number of Nodes là 2.\nKiểm tra tóm tắt cấu hình. Thay đổi Master user name thành admin\nNhập Master user password (Mật khẩu người dùng chính) Dưới phần Cluster permissions (Quyền cụm)\nNhấn Associate IAM role (Liên kết vai trò IAM)\nChọn Analyticsworkshop_RedshiftRole đã tạo trước đó\nNhấn Associate IAM roles (Liên kết vai trò IAM)\nAnalyticsworkshop_RedshiftRole sẽ xuất hiện dưới phần Associated IAM roles (Vai trò IAM liên kết) Để mặc định phần Additional configurations (Cấu hình bổ sung). Nó sử dụng VPC mặc định và nhóm bảo mật mặc định.\nNhấn Create cluster (Tạo cụm)\nTạo S3 Gateway Endpoint Truy cập vào: AWS VPC Console\nNhấn Create endpoint (Tạo điểm cuối)\nTên tag - tùy chọn: RedshiftS3EP\nChọn AWS Services dưới Service category (Chuyên mục dịch vụ) (mặc định) Dưới phần Service name (Tên dịch vụ), tìm kiếm \u0026ldquo;s3\u0026rdquo; và nhấn enter/return.\ncom.amazonaws.us-east-1.s3 sẽ xuất hiện như kết quả tìm kiếm. Chọn tùy chọn này với loại là Gateway\nDưới VPC, chọn VPC mặc định. Đây là VPC đã được sử dụng để cấu hình cụm Redshift. Sau khi kiểm tra lại ID VPC, tiếp tục cấu hình phần Route tables.\nChọn bảng định tuyến đã liệt kê (đây sẽ là bảng định tuyến chính). Bạn có thể kiểm tra điều này bằng cách xem Yes dưới cột Main.\nĐể mặc định Policy (Chính sách) là Full access (Toàn quyền). Tùy chọn thêm Tags, ví dụ: * workshop: AnalyticsOnAWS Nhấn Create endpoint (Tạo điểm cuối). Quá trình này sẽ mất vài giây để hoàn thành. Khi điểm cuối này đã sẵn sàng, bạn sẽ thấy trạng thái là - Available (Có sẵn) đối với điểm cuối S3 mới tạo.\nXác Minh và Thêm Quy Tắc vào Nhóm Bảo Mật Mặc Định Truy cập vào: VPC Security Groups\nChọn nhóm bảo mật Redshift. Nó sẽ là nhóm bảo mật mặc định nếu không thay đổi trong bước tạo cụm Redshift. Nhấn Edit inbound rules (Chỉnh sửa quy tắc đến) Nhấn Edit outbound rules (Chỉnh sửa quy tắc đi) Tạo Kết Nối Redshift dưới Kết Nối Glue Truy cập vào: Glue Connections Console\nDưới phần Connections (Kết nối), nhấn Create connection (Tạo kết nối) Đặt tên Connection name là analytics_workshop. Chọn Connection loại là Amazon Redshift. Cấu hình truy cập Kết Nối\nChọn Database instances là redshift-cluster-1 Tên cơ sở dữ liệu và Tên người dùng sẽ được tự động điền là dev và admin tương ứng. Mật khẩu: Nhập mật khẩu đã sử dụng khi thiết lập cụm Redshift. Nhấn Create connection (Tạo kết nối) Tạo Schema và Bảng Redshift Truy cập vào: Redshift Query Editor\nNhấn Connect (Kết nối) vào cơ sở dữ liệu Connection: Tạo kết nối mới Authentication: Temporary credentials (Chứng thực tạm thời) cluster: redshift-cluster-1 Database name: dev Database user: admin Nhấn Connect (Kết nối) Thực thi các truy vấn dưới đây để tạo schema và bảng cho dữ liệu thô và dữ liệu tham chiếu. CREATE schema redshift_lab; -- Tạo bảng f_raw_1. CREATE TABLE IF not EXISTS redshift_lab.f_raw_1 ( uuid varchar(256), device_ts timestamp, device_id int, device_temp int, track_id int, activity_type varchar(128), load_time int ); -- Tạo bảng d_ref_data_1. CREATE TABLE IF NOT EXISTS redshift_lab.d_ref_data_1 ( track_id int, track_name varchar(128), artist_name varchar(128) ); Sử dụng Jupyter Notebook trong AWS Glue để phát triển ETL tương tác Tải và lưu tệp này vào máy tính của bạn file\nTruy cập vào: Glue Studio jobs Chọn tùy chọn Jupyter Notebook Chọn Upload and edit an existing notebook Nhấn Choose file Duyệt và tải lên analytics-workshop-redshift-glueis-notebook.ipynb mà bạn đã tải về trước đó Nhấn Create Dưới phần Cài đặt Notebook và Cấu hình ban đầu\nTên job: AnalyticsOnAWS-Redshift IAM role: Analyticsworkshop-GlueISRole Để mặc định Kernel là Spark Nhấn Start notebook Thực thi các truy vấn dưới đây để kiểm tra số lượng bản ghi trong bảng dữ liệu thô và dữ liệu tham chiếu. select count(1) from redshift_lab.f_raw_1; select count(1) from redshift_lab.d_ref_data_1; select track_name, artist_name, count(1) frequency from redshift_lab.f_raw_1 fr inner join redshift_lab.d_ref_data_1 drf on fr.track_id = drf.track_id where activity_type = 'Running' group by track_name, artist_name order by frequency desc limit 10; "
},
{
	"uri": "http://Nam-An-work.github.io/vi/",
	"title": "Optimizing Data Pipelines for Real-time Analysis and Transformation on AWS",
	"tags": [],
	"description": "",
	"content": "Optimizing Data Pipelines for Real-time Analysis and Transformation on AWS Tổng quan về Workshop Workshop sẽ cung cấp cái nhìn tổng quan toàn diện về việc xây dựng một nền tảng phân tích trên AWS, bao phủ mỗi giai đoạn trong quy trình xử lý dữ liệu. Workshop sẽ khám phá cách hiệu quả để thu thập, lưu trữ, chuyển đổi và phân tích dữ liệu bằng cách sử dụng một loạt các dịch vụ AWS mạnh mẽ. Các dịch vụ chính như AWS Glue cho việc chuẩn bị dữ liệu, Amazon Athena để truy vấn dữ liệu, Amazon Kinesis cho truyền phát dữ liệu thời gian thực, Amazon EMR cho xử lý dữ liệu lớn, và Amazon QuickSight để trực quan hóa dữ liệu sẽ được thảo luận. Ngoài ra, AWS Lambda và Amazon Redshift cũng sẽ được giới thiệu, cung cấp các giải pháp tính toán không máy chủ và kho dữ liệu, giúp xây dựng các hệ thống phân tích mạnh mẽ và có khả năng mở rộng.\nNội dung Giới thiệu Nhập và lưu trữ Dữ liệu danh mục Chuyển đổi dữ liệu Phân tích với Athena Phân tích với Kinesis Data Analytics Quícksight Phục vụ với Lambda Kho dữ liệu trên Redshift Dọn dẹp tài nguyên "
},
{
	"uri": "http://Nam-An-work.github.io/vi/10-cleanup/",
	"title": "10. Dọn dẹp tài nguyên  ",
	"tags": [],
	"description": "",
	"content": "Chúng ta sẽ thực hiện các bước sau để xóa các tài nguyên đã tạo trong bài tập này.\nKinesis Firehose Delivery Stream Truy cập vào: Kinesis Console Click me\nXóa Firehose: analytics-workshop-stream Kinesis Data Stream Truy cập vào: Kinesis Console Click me\nXóa Data Stream: analytics-workshop-data-stream Kinesis Data Analytics Studio Notebook Truy cập vào: Kinesis Console Click me\nXóa Notebook: AnalyticsWorkshop-KDANotebook Lambda Truy cập vào: Lambda Console Click me\nĐi tới danh sách các hàm và chọn Analyticsworkshop_top5Songs. Dưới menu Actions, chọn Delete. Glue Database Truy cập vào: Glue Console Click me\nXóa Database: analyticsworkshopdb Glue Crawler Truy cập vào: Glue Crawlers Click me\nXóa Crawler: AnalyticsworkshopCrawler Glue Studio Job Truy cập vào: Glue Studio Job Click me\nChọn AnalyticsOnAWS-GlueStudio Chọn AnalyticsOnAWS-GlueIS Chọn AnalyticsOnAWS-Redshift Nhấn Action và chọn Delete job(s) Glue DataBrew projects Truy cập vào: Glue DataBrew projects Click me\nChọn AnalyticsOnAWS-GlueDataBrew\nNhấn Action và chọn Delete\nChọn Delete attached recipe và nhấn Delete Glue DataBrew datasets Truy cập vào: Glue DataBrew datasetsClick me\nChọn tên dataset: reference-data-dataset và raw-dataset\nNhấn Action và chọn Delete\nXác nhận xóa bằng cách nhấn Delete Glue DataBrew Jobs Truy cập vào: Glue DataBrew JobsClick me\nChọn tên dataset: raw-dataset profile job\nNhấn Action và chọn Delete\nXác nhận xóa bằng cách nhấn Delete Xóa Glue connection Truy cập vào: Glue Connections Click me\nChọn analytics_workshop\nTừ Actions, chọn Delete Connection\nNhấn Delete Xóa IAM Role Truy cập vào: IAM Console Click me\nTìm kiếm các role sau và xóa: Analyticsworkshop-GlueISRole\nAnalyticsworkshop_RedshiftRole\nAnalyticsworkshopKinesisAnalyticsRole\nAnalyticsworkshop_top5Songs-role- Xóa IAM Policy Truy cập vào: IAM Console Click me\nTìm kiếm các policy sau và xóa: AWSGlueInteractiveSessionPassRolePolicy Xóa Redshift cluster Truy cập vào: Redshift Console Click me\nChọn redshift-cluster-1\nTừ menu Actions, chọn Delete.\nBỏ chọn Create final snapshot\nNhấn Delete Xóa S3 Gateway Endpoint Truy cập vào: S3 Gateway Endpoints Click me\nChọn RedshiftS3EP\nTừ Actions, chọn Delete Endpoint Hoàn tác các quy tắc của Security Group Truy cập vào: EC2 Security Groups Click me\nVới security group mặc định:\nNhấn vào Inbound Rules\nNhấn Edit Rules\nXóa dòng với S3 prefix list ID\nNhấn Save rules\nNhấn vào Outbound rules\nNhấn Edit Rules\nXóa quy tắc All TCP tự tham chiếu.\nNhấn Save rules Xóa S3 bucket Truy cập vào: S3 Console Click me\nXóa Bucket: yourname-analytics-workshop-bucket\nBạn có thể cần phải dọn dẹp bucket trước khi xóa.\nSau khi dọn dẹp, tiến hành xóa bucket Xóa Cognito CloudFormation Stack Truy cập vào: CloudFormation Click me\nNhấn: Kinesis-Data-Generator-Cognito-User\nNhấn: Actions \u0026gt; DeleteStack\nTrên màn hình xác nhận:\nNhấn Delete Đóng tài khoản QuickSight Truy cập vào: Quicksight Console Click me\nNhấn: Unsubscribe Xóa Cognito Userpool Truy cập vào: Cognito Console Click me\nNhấn Kinesis Data-Generator Users\nNhấn Delete Pool "
},
{
	"uri": "http://Nam-An-work.github.io/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://Nam-An-work.github.io/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]